{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install beautifulsoup4\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport requests","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-21T20:54:21.804252Z","iopub.execute_input":"2024-07-21T20:54:21.804659Z","iopub.status.idle":"2024-07-21T20:54:40.245472Z","shell.execute_reply.started":"2024-07-21T20:54:21.804624Z","shell.execute_reply":"2024-07-21T20:54:40.244127Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n","output_type":"stream"}]},{"cell_type":"code","source":"url = 'https://en.wikipedia.org/wiki/Large_language_model'\n\npage = requests.get(url)\n\n# Parsing the HTML content using BeautifulSoup\nsoup = BeautifulSoup(page.text, \"html\")\n\nList = soup.find_all(\"table\")[2]\nList","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:57:09.431890Z","iopub.execute_input":"2024-07-21T20:57:09.432386Z","iopub.status.idle":"2024-07-21T20:57:10.263536Z","shell.execute_reply.started":"2024-07-21T20:57:09.432344Z","shell.execute_reply":"2024-07-21T20:57:10.262230Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<table class=\"wikitable sortable\">\n<tbody><tr>\n<th>Name</th>\n<th>Release date<sup class=\"reference\" id=\"cite_ref-133\"><a href=\"#cite_note-133\">[a]</a></sup></th>\n<th>Developer</th>\n<th>Number of parameters (billion) <sup class=\"reference\" id=\"cite_ref-134\"><a href=\"#cite_note-134\">[b]</a></sup></th>\n<th>Corpus size\n</th>\n<th>Training cost (petaFLOP-day)</th>\n<th>License<sup class=\"reference\" id=\"cite_ref-135\"><a href=\"#cite_note-135\">[c]</a></sup></th>\n<th>Notes\n</th></tr>\n<tr>\n<td><a href=\"/wiki/GPT-1\" title=\"GPT-1\">GPT-1</a></td>\n<td><span data-sort-value=\"000000002018-06-01-0000\" style=\"white-space:nowrap\">June 2018</span></td>\n<td><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></td>\n<td><span data-sort-value=\"117000000 !\">0.117</span></td>\n<td>\n</td>\n<td>1<sup class=\"reference\" id=\"cite_ref-oai-unsup_136-0\"><a href=\"#cite_note-oai-unsup-136\">[133]</a></sup></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">MIT<sup class=\"reference\" id=\"cite_ref-gpt1_137-0\"><a href=\"#cite_note-gpt1-137\">[134]</a></sup>\n</td>\n<td>First GPT model, decoder-only transformer. Trained for 30 days on 8 P600 <a href=\"/wiki/Graphics_processing_unit\" title=\"Graphics processing unit\">GPUs</a>.\n</td></tr>\n<tr>\n<td><a href=\"/wiki/BERT_(language_model)\" title=\"BERT (language model)\">BERT</a></td>\n<td><span data-sort-value=\"000000002018-10-01-0000\" style=\"white-space:nowrap\">October 2018</span></td>\n<td><a href=\"/wiki/Google\" title=\"Google\">Google</a></td>\n<td><span data-sort-value=\"340000000 !\">0.340</span><sup class=\"reference\" id=\"cite_ref-bert-paper_138-0\"><a href=\"#cite_note-bert-paper-138\">[135]</a></sup></td>\n<td><span data-sort-value=\"3300000000 !\">3.3 billion</span> words<sup class=\"reference\" id=\"cite_ref-bert-paper_138-1\"><a href=\"#cite_note-bert-paper-138\">[135]</a></sup>\n</td>\n<td><span data-sort-value=\"9 !\">9</span><sup class=\"reference\" id=\"cite_ref-bHZJ2_139-0\"><a href=\"#cite_note-bHZJ2-139\">[136]</a></sup></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0<sup class=\"reference\" id=\"cite_ref-bert-web_140-0\"><a href=\"#cite_note-bert-web-140\">[137]</a></sup>\n</td>\n<td>An early and influential language model,<sup class=\"reference\" id=\"cite_ref-Manning-2022_5-1\"><a href=\"#cite_note-Manning-2022-5\">[5]</a></sup> but encoder-only and thus not built to be prompted or generative<sup class=\"reference\" id=\"cite_ref-Ir545_141-0\"><a href=\"#cite_note-Ir545-141\">[138]</a></sup>\n</td></tr>\n<tr>\n<td><a href=\"/wiki/T5_(language_model)\" title=\"T5 (language model)\">T5</a>\n</td>\n<td><span data-sort-value=\"000000002019-10-01-0000\" style=\"white-space:nowrap\">October 2019</span>\n</td>\n<td>Google\n</td>\n<td>11<sup class=\"reference\" id=\"cite_ref-:6_142-0\"><a href=\"#cite_note-:6-142\">[139]</a></sup>\n</td>\n<td>34 billion tokens<sup class=\"reference\" id=\"cite_ref-:6_142-1\"><a href=\"#cite_note-:6-142\">[139]</a></sup>\n</td>\n<td>\n</td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0<sup class=\"reference\" id=\"cite_ref-143\"><a href=\"#cite_note-143\">[140]</a></sup>\n</td>\n<td>Base model for many Google projects, such as Imagen.<sup class=\"reference\" id=\"cite_ref-144\"><a href=\"#cite_note-144\">[141]</a></sup>\n</td></tr>\n<tr>\n<td>XLNet</td>\n<td><span data-sort-value=\"000000002019-06-01-0000\" style=\"white-space:nowrap\">June 2019</span></td>\n<td><a href=\"/wiki/Google\" title=\"Google\">Google</a></td>\n<td><span data-sort-value=\"340000000 !\">~0.340</span><sup class=\"reference\" id=\"cite_ref-45rAm_145-0\"><a href=\"#cite_note-45rAm-145\">[142]</a></sup></td>\n<td><span data-sort-value=\"3300000000 !\">33</span> billion words\n</td>\n<td></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0<sup class=\"reference\" id=\"cite_ref-xlnet_146-0\"><a href=\"#cite_note-xlnet-146\">[143]</a></sup>\n</td>\n<td>An alternative to BERT; designed as encoder-only<sup class=\"reference\" id=\"cite_ref-gAbNO_147-0\"><a href=\"#cite_note-gAbNO-147\">[144]</a></sup><sup class=\"reference\" id=\"cite_ref-LX3rI_148-0\"><a href=\"#cite_note-LX3rI-148\">[145]</a></sup>\n</td></tr>\n<tr>\n<td><a href=\"/wiki/GPT-2\" title=\"GPT-2\">GPT-2</a></td>\n<td><span data-sort-value=\"000000002019-02-01-0000\" style=\"white-space:nowrap\">February 2019</span></td>\n<td><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></td>\n<td><span data-sort-value=\"1500000000 !\">1.5</span><sup class=\"reference\" id=\"cite_ref-15Brelease_149-0\"><a href=\"#cite_note-15Brelease-149\">[146]</a></sup></td>\n<td>40GB<sup class=\"reference\" id=\"cite_ref-5T8u5_150-0\"><a href=\"#cite_note-5T8u5-150\">[147]</a></sup> (~<span data-sort-value=\"10000000000 !\">10 billion</span> tokens)<sup class=\"reference\" id=\"cite_ref-LambdaLabs_151-0\"><a href=\"#cite_note-LambdaLabs-151\">[148]</a></sup>\n</td>\n<td></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">MIT<sup class=\"reference\" id=\"cite_ref-Sudbe_152-0\"><a href=\"#cite_note-Sudbe-152\">[149]</a></sup>\n</td>\n<td>general-purpose model based on transformer architecture\n</td></tr>\n<tr>\n<td><a href=\"/wiki/GPT-3\" title=\"GPT-3\">GPT-3</a></td>\n<td><span data-sort-value=\"000000002020-05-01-0000\" style=\"white-space:nowrap\">May 2020</span></td>\n<td>OpenAI</td>\n<td><span data-sort-value=\"175000000000 !\">175</span><sup class=\"reference\" id=\"cite_ref-Wiggers_46-1\"><a href=\"#cite_note-Wiggers-46\">[46]</a></sup></td>\n<td><span data-sort-value=\"300000000000 !\">300 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-LambdaLabs_151-1\"><a href=\"#cite_note-LambdaLabs-151\">[148]</a></sup>\n</td>\n<td>3640<sup class=\"reference\" id=\"cite_ref-:2_153-0\"><a href=\"#cite_note-:2-153\">[150]</a></sup></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">proprietary\n</td>\n<td>A fine-tuned variant of GPT-3, termed GPT-3.5, was made available to the public through a web interface called <a href=\"/wiki/ChatGPT\" title=\"ChatGPT\">ChatGPT</a> in 2022.<sup class=\"reference\" id=\"cite_ref-chatgpt-blog_154-0\"><a href=\"#cite_note-chatgpt-blog-154\">[151]</a></sup>\n</td></tr>\n<tr>\n<td>GPT-Neo</td>\n<td><span data-sort-value=\"000000002021-03-01-0000\" style=\"white-space:nowrap\">March 2021</span></td>\n<td><a href=\"/wiki/EleutherAI\" title=\"EleutherAI\">EleutherAI</a></td>\n<td><span data-sort-value=\"2700000000 !\">2.7</span><sup class=\"reference\" id=\"cite_ref-gpt-neo_155-0\"><a href=\"#cite_note-gpt-neo-155\">[152]</a></sup></td>\n<td>825 GiB<sup class=\"reference\" id=\"cite_ref-Pile_156-0\"><a href=\"#cite_note-Pile-156\">[153]</a></sup>\n</td>\n<td></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">MIT<sup class=\"reference\" id=\"cite_ref-vb-gpt-neo_157-0\"><a href=\"#cite_note-vb-gpt-neo-157\">[154]</a></sup>\n</td>\n<td>The first of <a href=\"/wiki/EleutherAI#GPT_models\" title=\"EleutherAI\">a series of free GPT-3 alternatives</a> released by EleutherAI. GPT-Neo outperformed an equivalent-size GPT-3 model on some benchmarks, but was significantly worse than the largest GPT-3.<sup class=\"reference\" id=\"cite_ref-vb-gpt-neo_157-1\"><a href=\"#cite_note-vb-gpt-neo-157\">[154]</a></sup>\n</td></tr>\n<tr>\n<td><a href=\"/wiki/GPT-J\" title=\"GPT-J\">GPT-J</a></td>\n<td><span data-sort-value=\"000000002021-06-01-0000\" style=\"white-space:nowrap\">June 2021</span></td>\n<td><a href=\"/wiki/EleutherAI\" title=\"EleutherAI\">EleutherAI</a></td>\n<td><span data-sort-value=\"6000000000 !\">6</span><sup class=\"reference\" id=\"cite_ref-JxohJ_158-0\"><a href=\"#cite_note-JxohJ-158\">[155]</a></sup></td>\n<td>825 GiB<sup class=\"reference\" id=\"cite_ref-Pile_156-1\"><a href=\"#cite_note-Pile-156\">[153]</a></sup>\n</td>\n<td>200<sup class=\"reference\" id=\"cite_ref-:3_159-0\"><a href=\"#cite_note-:3-159\">[156]</a></sup></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n</td>\n<td>GPT-3-style language model\n</td></tr>\n<tr>\n<td>Megatron-Turing NLG</td>\n<td><span data-sort-value=\"000000002021-10-01-0000\" style=\"white-space:nowrap\">October 2021</span><sup class=\"reference\" id=\"cite_ref-BwnW5_160-0\"><a href=\"#cite_note-BwnW5-160\">[157]</a></sup></td>\n<td><a href=\"/wiki/Microsoft\" title=\"Microsoft\">Microsoft</a> and <a href=\"/wiki/Nvidia\" title=\"Nvidia\">Nvidia</a></td>\n<td><span data-sort-value=\"530000000000 !\">530</span><sup class=\"reference\" id=\"cite_ref-mtnlg-preprint_161-0\"><a href=\"#cite_note-mtnlg-preprint-161\">[158]</a></sup></td>\n<td><span data-sort-value=\"338600000000 !\">338.6 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-mtnlg-preprint_161-1\"><a href=\"#cite_note-mtnlg-preprint-161\">[158]</a></sup>\n</td>\n<td></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Restricted web access\n</td>\n<td>Standard architecture but trained on a supercomputing cluster.\n</td></tr>\n<tr>\n<td>Ernie 3.0 Titan</td>\n<td><span data-sort-value=\"000000002021-12-01-0000\" style=\"white-space:nowrap\">December 2021</span></td>\n<td><a href=\"/wiki/Baidu\" title=\"Baidu\">Baidu</a></td>\n<td><span data-sort-value=\"260000000000 !\">260</span><sup class=\"reference\" id=\"cite_ref-qeOB8_162-0\"><a href=\"#cite_note-qeOB8-162\">[159]</a></sup></td>\n<td>4 Tb\n</td>\n<td></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Chinese-language LLM. <a href=\"/wiki/Ernie_Bot\" title=\"Ernie Bot\">Ernie Bot</a> is based on this model.\n</td></tr>\n<tr>\n<td><a href=\"/wiki/Claude_(language_model)\" title=\"Claude (language model)\">Claude</a><sup class=\"reference\" id=\"cite_ref-i8jc4_163-0\"><a href=\"#cite_note-i8jc4-163\">[160]</a></sup></td>\n<td><span data-sort-value=\"000000002021-12-01-0000\" style=\"white-space:nowrap\">December 2021</span></td>\n<td><a href=\"/wiki/Anthropic\" title=\"Anthropic\">Anthropic</a></td>\n<td><span data-sort-value=\"52000000000 !\">52</span><sup class=\"reference\" id=\"cite_ref-AnthroArch_164-0\"><a href=\"#cite_note-AnthroArch-164\">[161]</a></sup></td>\n<td><span data-sort-value=\"400000000000 !\">400 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-AnthroArch_164-1\"><a href=\"#cite_note-AnthroArch-164\">[161]</a></sup>\n</td>\n<td></td>\n<td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">beta\n</td>\n<td>Fine-tuned for desirable behavior in conversations.<sup class=\"reference\" id=\"cite_ref-RZqhw_165-0\"><a href=\"#cite_note-RZqhw-165\">[162]</a></sup>\n</td></tr>\n<tr>\n<td>GLaM (Generalist Language Model)</td>\n<td><span data-sort-value=\"000000002021-12-01-0000\" style=\"white-space:nowrap\">December 2021</span></td>\n<td>Google</td>\n<td><span data-sort-value=\"1200000000000 !\">1200</span><sup class=\"reference\" id=\"cite_ref-glam-blog_37-1\"><a href=\"#cite_note-glam-blog-37\">[37]</a></sup></td>\n<td><span data-sort-value=\"1600000000000 !\">1.6 trillion</span> tokens<sup class=\"reference\" id=\"cite_ref-glam-blog_37-2\"><a href=\"#cite_note-glam-blog-37\">[37]</a></sup>\n</td>\n<td>5600<sup class=\"reference\" id=\"cite_ref-glam-blog_37-3\"><a href=\"#cite_note-glam-blog-37\">[37]</a></sup></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Sparse <a href=\"/wiki/Mixture_of_experts\" title=\"Mixture of experts\">mixture of experts</a> model, making it more expensive to train but cheaper to run inference compared to GPT-3.\n</td></tr>\n<tr>\n<td>Gopher</td>\n<td><span data-sort-value=\"000000002021-12-01-0000\" style=\"white-space:nowrap\">December 2021</span></td>\n<td><a class=\"mw-redirect\" href=\"/wiki/DeepMind\" title=\"DeepMind\">DeepMind</a></td>\n<td><span data-sort-value=\"280000000000 !\">280</span><sup class=\"reference\" id=\"cite_ref-mD5eE_166-0\"><a href=\"#cite_note-mD5eE-166\">[163]</a></sup></td>\n<td><span data-sort-value=\"300000000000 !\">300 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-hoffman_167-0\"><a href=\"#cite_note-hoffman-167\">[164]</a></sup>\n</td>\n<td>5833<sup class=\"reference\" id=\"cite_ref-:4_168-0\"><a href=\"#cite_note-:4-168\">[165]</a></sup></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Later developed into the Chinchilla model.\n</td></tr>\n<tr>\n<td><a href=\"/wiki/LaMDA\" title=\"LaMDA\">LaMDA</a> (Language Models for Dialog Applications)</td>\n<td><span data-sort-value=\"000000002022-01-01-0000\" style=\"white-space:nowrap\">January 2022</span></td>\n<td>Google</td>\n<td><span data-sort-value=\"137000000000 !\">137</span><sup class=\"reference\" id=\"cite_ref-lamda-blog_169-0\"><a href=\"#cite_note-lamda-blog-169\">[166]</a></sup></td>\n<td>1.56T words,<sup class=\"reference\" id=\"cite_ref-lamda-blog_169-1\"><a href=\"#cite_note-lamda-blog-169\">[166]</a></sup> <span data-sort-value=\"168000000000 !\">168 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-hoffman_167-1\"><a href=\"#cite_note-hoffman-167\">[164]</a></sup>\n</td>\n<td>4110<sup class=\"reference\" id=\"cite_ref-DMs9Z_170-0\"><a href=\"#cite_note-DMs9Z-170\">[167]</a></sup></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Specialized for response generation in conversations.\n</td></tr>\n<tr>\n<td>GPT-NeoX</td>\n<td><span data-sort-value=\"000000002022-02-01-0000\" style=\"white-space:nowrap\">February 2022</span></td>\n<td><a href=\"/wiki/EleutherAI\" title=\"EleutherAI\">EleutherAI</a></td>\n<td><span data-sort-value=\"20000000000 !\">20</span><sup class=\"reference\" id=\"cite_ref-gpt-neox-20b_171-0\"><a href=\"#cite_note-gpt-neox-20b-171\">[168]</a></sup></td>\n<td>825 GiB<sup class=\"reference\" id=\"cite_ref-Pile_156-2\"><a href=\"#cite_note-Pile-156\">[153]</a></sup>\n</td>\n<td>740<sup class=\"reference\" id=\"cite_ref-:3_159-1\"><a href=\"#cite_note-:3-159\">[156]</a></sup></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n</td>\n<td>based on the Megatron architecture\n</td></tr>\n<tr>\n<td><a class=\"mw-redirect\" href=\"/wiki/Chinchilla_AI\" title=\"Chinchilla AI\">Chinchilla</a></td>\n<td><span data-sort-value=\"000000002022-03-01-0000\" style=\"white-space:nowrap\">March 2022</span></td>\n<td><a class=\"mw-redirect\" href=\"/wiki/DeepMind\" title=\"DeepMind\">DeepMind</a></td>\n<td><span data-sort-value=\"70000000000 !\">70</span><sup class=\"reference\" id=\"cite_ref-chinchilla-blog_172-0\"><a href=\"#cite_note-chinchilla-blog-172\">[169]</a></sup></td>\n<td><span data-sort-value=\"1400000000000 !\">1.4 trillion</span> tokens<sup class=\"reference\" id=\"cite_ref-chinchilla-blog_172-1\"><a href=\"#cite_note-chinchilla-blog-172\">[169]</a></sup><sup class=\"reference\" id=\"cite_ref-hoffman_167-2\"><a href=\"#cite_note-hoffman-167\">[164]</a></sup>\n</td>\n<td>6805<sup class=\"reference\" id=\"cite_ref-:4_168-1\"><a href=\"#cite_note-:4-168\">[165]</a></sup></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Reduced-parameter model trained on more data. Used in the <a class=\"mw-redirect\" href=\"/wiki/Sparrow_(bot)\" title=\"Sparrow (bot)\">Sparrow</a> bot. Often cited for its <a href=\"/wiki/Neural_scaling_law\" title=\"Neural scaling law\">neural scaling law</a>.\n</td></tr>\n<tr>\n<td><a href=\"/wiki/PaLM\" title=\"PaLM\">PaLM</a> (Pathways Language Model)</td>\n<td><span data-sort-value=\"000000002022-04-01-0000\" style=\"white-space:nowrap\">April 2022</span></td>\n<td>Google</td>\n<td><span data-sort-value=\"540000000000 !\">540</span><sup class=\"reference\" id=\"cite_ref-palm-blog_173-0\"><a href=\"#cite_note-palm-blog-173\">[170]</a></sup></td>\n<td><span data-sort-value=\"768000000000 !\">768 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-chinchilla-blog_172-2\"><a href=\"#cite_note-chinchilla-blog-172\">[169]</a></sup>\n</td>\n<td>29250<sup class=\"reference\" id=\"cite_ref-:4_168-2\"><a href=\"#cite_note-:4-168\">[165]</a></sup></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Trained for ~60 days on ~6000 <a href=\"/wiki/Tensor_Processing_Unit\" title=\"Tensor Processing Unit\">TPU v4</a> chips.<sup class=\"reference\" id=\"cite_ref-:4_168-3\"><a href=\"#cite_note-:4-168\">[165]</a></sup>\n</td></tr>\n<tr>\n<td>OPT (Open Pretrained Transformer)</td>\n<td><span data-sort-value=\"000000002022-05-01-0000\" style=\"white-space:nowrap\">May 2022</span></td>\n<td><a href=\"/wiki/Meta_Platforms\" title=\"Meta Platforms\">Meta</a></td>\n<td><span data-sort-value=\"175000000000 !\">175</span><sup class=\"reference\" id=\"cite_ref-jlof8_174-0\"><a href=\"#cite_note-jlof8-174\">[171]</a></sup></td>\n<td><span data-sort-value=\"180000000000 !\">180 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-QjTIc_175-0\"><a href=\"#cite_note-QjTIc-175\">[172]</a></sup>\n</td>\n<td>310<sup class=\"reference\" id=\"cite_ref-:3_159-2\"><a href=\"#cite_note-:3-159\">[156]</a></sup></td>\n<td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">Non-commercial research<sup class=\"reference\" id=\"cite_ref-176\"><a href=\"#cite_note-176\">[d]</a></sup>\n</td>\n<td>GPT-3 architecture with some adaptations from Megatron\n</td></tr>\n<tr>\n<td>YaLM 100B</td>\n<td><span data-sort-value=\"000000002022-06-01-0000\" style=\"white-space:nowrap\">June 2022</span></td>\n<td><a href=\"/wiki/Yandex\" title=\"Yandex\">Yandex</a></td>\n<td><span data-sort-value=\"100000000000 !\">100</span><sup class=\"reference\" id=\"cite_ref-yalm-repo_177-0\"><a href=\"#cite_note-yalm-repo-177\">[173]</a></sup>\n</td>\n<td>1.7TB<sup class=\"reference\" id=\"cite_ref-yalm-repo_177-1\"><a href=\"#cite_note-yalm-repo-177\">[173]</a></sup></td>\n<td></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0</td>\n<td>English-Russian model based on Microsoft's Megatron-LM.\n</td></tr>\n<tr>\n<td>Minerva</td>\n<td><span data-sort-value=\"000000002022-06-01-0000\" style=\"white-space:nowrap\">June 2022</span></td>\n<td>Google</td>\n<td><span data-sort-value=\"540000000000 !\">540</span><sup class=\"reference\" id=\"cite_ref-minerva-paper_178-0\"><a href=\"#cite_note-minerva-paper-178\">[174]</a></sup></td>\n<td>38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server<sup class=\"reference\" id=\"cite_ref-minerva-paper_178-1\"><a href=\"#cite_note-minerva-paper-178\">[174]</a></sup>\n</td>\n<td></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>For solving \"mathematical and scientific questions using step-by-step reasoning\".<sup class=\"reference\" id=\"cite_ref-FfCNK_179-0\"><a href=\"#cite_note-FfCNK-179\">[175]</a></sup> Based on PaLM model, further trained on mathematical and scientific data.\n</td></tr>\n<tr>\n<td><a href=\"/wiki/BLOOM_(language_model)\" title=\"BLOOM (language model)\">BLOOM</a></td>\n<td><span data-sort-value=\"000000002022-07-01-0000\" style=\"white-space:nowrap\">July 2022</span></td>\n<td>Large collaboration led by <a href=\"/wiki/Hugging_Face\" title=\"Hugging Face\">Hugging Face</a></td>\n<td><span data-sort-value=\"175000000000 !\">175</span><sup class=\"reference\" id=\"cite_ref-bigger-better_180-0\"><a href=\"#cite_note-bigger-better-180\">[176]</a></sup></td>\n<td><span data-sort-value=\"350000000000 !\">350 billion</span> tokens (1.6TB)<sup class=\"reference\" id=\"cite_ref-B8wB2_181-0\"><a href=\"#cite_note-B8wB2-181\">[177]</a></sup>\n</td>\n<td></td>\n<td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">Responsible AI\n</td>\n<td>Essentially GPT-3 but trained on a multi-lingual corpus (30% English excluding programming languages)\n</td></tr>\n<tr>\n<td>Galactica</td>\n<td><span data-sort-value=\"000000002022-11-01-0000\" style=\"white-space:nowrap\">November 2022</span></td>\n<td><a href=\"/wiki/Meta_Platforms\" title=\"Meta Platforms\">Meta</a></td>\n<td><span data-sort-value=\"120000000000 !\">120</span></td>\n<td><span data-sort-value=\"350000000000 !\">106 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-37sY6_182-0\"><a href=\"#cite_note-37sY6-182\">[178]</a></sup>\n</td>\n<td>unknown</td>\n<td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">CC-BY-NC-4.0\n</td>\n<td>Trained on scientific text and modalities.\n</td></tr>\n<tr>\n<td>AlexaTM (Teacher Models)</td>\n<td><span data-sort-value=\"000000002022-11-01-0000\" style=\"white-space:nowrap\">November 2022</span></td>\n<td><a href=\"/wiki/Amazon_(company)\" title=\"Amazon (company)\">Amazon</a></td>\n<td><span data-sort-value=\"20000000000 !\">20</span><sup class=\"reference\" id=\"cite_ref-u5szh_183-0\"><a href=\"#cite_note-u5szh-183\">[179]</a></sup></td>\n<td><span data-sort-value=\"1300000000000 !\">1.3 trillion</span><sup class=\"reference\" id=\"cite_ref-HaA7l_184-0\"><a href=\"#cite_note-HaA7l-184\">[180]</a></sup>\n</td>\n<td></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">proprietary<sup class=\"reference\" id=\"cite_ref-rpehM_185-0\"><a href=\"#cite_note-rpehM-185\">[181]</a></sup>\n</td>\n<td>bidirectional sequence-to-sequence architecture\n</td></tr>\n<tr>\n<td><a href=\"/wiki/Neuro-sama\" title=\"Neuro-sama\">Neuro-sama</a></td>\n<td><span data-sort-value=\"000000002022-12-01-0000\" style=\"white-space:nowrap\">December 2022</span></td>\n<td>Independent</td>\n<td>Unknown</td>\n<td>Unknown\n</td>\n<td></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">privately-owned\n</td>\n<td>A language model designed for live-streaming on <a href=\"/wiki/Twitch_(service)\" title=\"Twitch (service)\">Twitch</a>.\n</td></tr>\n<tr>\n<td><a class=\"mw-redirect\" href=\"/wiki/LLaMA\" title=\"LLaMA\">LLaMA</a> (Large Language Model Meta AI)</td>\n<td><span data-sort-value=\"000000002023-02-01-0000\" style=\"white-space:nowrap\">February 2023</span></td>\n<td><a href=\"/wiki/Meta_AI\" title=\"Meta AI\">Meta AI</a></td>\n<td><span data-sort-value=\"65000000000 !\">65</span><sup class=\"reference\" id=\"cite_ref-llama-blog_186-0\"><a href=\"#cite_note-llama-blog-186\">[182]</a></sup></td>\n<td><span data-sort-value=\"1400000000000 !\">1.4 trillion</span><sup class=\"reference\" id=\"cite_ref-llama-blog_186-1\"><a href=\"#cite_note-llama-blog-186\">[182]</a></sup>\n</td>\n<td>6300<sup class=\"reference\" id=\"cite_ref-:5_187-0\"><a href=\"#cite_note-:5-187\">[183]</a></sup></td>\n<td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">Non-commercial research<sup class=\"reference\" id=\"cite_ref-188\"><a href=\"#cite_note-188\">[e]</a></sup>\n</td>\n<td>Corpus has 20 languages. \"Overtrained\" (compared to <a href=\"/wiki/Chinchilla_(language_model)\" title=\"Chinchilla (language model)\">Chinchilla scaling law</a>) for better performance with fewer parameters.<sup class=\"reference\" id=\"cite_ref-llama-blog_186-2\"><a href=\"#cite_note-llama-blog-186\">[182]</a></sup>\n</td></tr>\n<tr>\n<td><a href=\"/wiki/GPT-4\" title=\"GPT-4\">GPT-4</a></td>\n<td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span></td>\n<td>OpenAI</td>\n<td>Unknown<sup class=\"reference\" id=\"cite_ref-190\"><a href=\"#cite_note-190\">[f]</a></sup></td>\n<td>Unknown\n</td>\n<td>Unknown</td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">proprietary\n</td>\n<td>Available for ChatGPT Plus users and used in <a href=\"/wiki/GPT-4#Usage\" title=\"GPT-4\">several products</a>.\n</td></tr>\n<tr>\n<td>Cerebras-GPT\n</td>\n<td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span>\n</td>\n<td><a href=\"/wiki/Cerebras\" title=\"Cerebras\">Cerebras</a>\n</td>\n<td><span data-sort-value=\"13000000000 !\">13</span><sup class=\"reference\" id=\"cite_ref-D0k2a_191-0\"><a href=\"#cite_note-D0k2a-191\">[185]</a></sup>\n</td>\n<td>\n</td>\n<td>270<sup class=\"reference\" id=\"cite_ref-:3_159-3\"><a href=\"#cite_note-:3-159\">[156]</a></sup></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n</td>\n<td>Trained with <a href=\"/wiki/Chinchilla_(language_model)\" title=\"Chinchilla (language model)\">Chinchilla formula</a>.\n</td></tr>\n<tr>\n<td>Falcon</td>\n<td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span></td>\n<td><a href=\"/wiki/Technology_Innovation_Institute\" title=\"Technology Innovation Institute\">Technology Innovation Institute</a></td>\n<td><span data-sort-value=\"40000000000 !\">40</span><sup class=\"reference\" id=\"cite_ref-falcon_192-0\"><a href=\"#cite_note-falcon-192\">[186]</a></sup></td>\n<td>1 trillion tokens, from RefinedWeb (filtered web text corpus)<sup class=\"reference\" id=\"cite_ref-Xb1gq_193-0\"><a href=\"#cite_note-Xb1gq-193\">[187]</a></sup> plus some \"curated corpora\".<sup class=\"reference\" id=\"cite_ref-gzTNw_194-0\"><a href=\"#cite_note-gzTNw-194\">[188]</a></sup>\n</td>\n<td>2800<sup class=\"reference\" id=\"cite_ref-:5_187-1\"><a href=\"#cite_note-:5-187\">[183]</a></sup></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0<sup class=\"reference\" id=\"cite_ref-Wmlcs_195-0\"><a href=\"#cite_note-Wmlcs-195\">[189]</a></sup>\n</td>\n<td>\n</td></tr>\n<tr>\n<td>BloombergGPT</td>\n<td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span></td>\n<td><a href=\"/wiki/Bloomberg_L.P.\" title=\"Bloomberg L.P.\">Bloomberg L.P.</a></td>\n<td><span data-sort-value=\"50000000000 !\">50</span></td>\n<td>363 billion token dataset based on Bloomberg's data sources, plus 345 billion tokens from general purpose datasets<sup class=\"reference\" id=\"cite_ref-nGOSu_196-0\"><a href=\"#cite_note-nGOSu-196\">[190]</a></sup>\n</td>\n<td></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Trained on financial data from proprietary sources, for financial tasks.\n</td></tr>\n<tr>\n<td><a href=\"/wiki/Huawei_PanGu\" title=\"Huawei PanGu\">PanGu-Σ</a></td>\n<td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span></td>\n<td><a href=\"/wiki/Huawei\" title=\"Huawei\">Huawei</a></td>\n<td><span data-sort-value=\"1085000000000 !\">1085</span></td>\n<td>329 billion tokens<sup class=\"reference\" id=\"cite_ref-9WSFw_197-0\"><a href=\"#cite_note-9WSFw-197\">[191]</a></sup>\n</td>\n<td></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>\n</td></tr>\n<tr>\n<td>OpenAssistant<sup class=\"reference\" id=\"cite_ref-JiOl8_198-0\"><a href=\"#cite_note-JiOl8-198\">[192]</a></sup></td>\n<td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span></td>\n<td><a href=\"/wiki/LAION\" title=\"LAION\">LAION</a></td>\n<td><span data-sort-value=\"17000000000 !\">17</span></td>\n<td>1.5 trillion tokens\n</td>\n<td></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n</td>\n<td>Trained on crowdsourced open data\n</td></tr>\n<tr>\n<td>Jurassic-2<sup class=\"reference\" id=\"cite_ref-199\"><a href=\"#cite_note-199\">[193]</a></sup>\n</td>\n<td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span>\n</td>\n<td><a href=\"/wiki/AI21_Labs\" title=\"AI21 Labs\">AI21 Labs</a>\n</td>\n<td>Unknown\n</td>\n<td>Unknown\n</td>\n<td></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Multilingual<sup class=\"reference\" id=\"cite_ref-200\"><a href=\"#cite_note-200\">[194]</a></sup>\n</td></tr>\n<tr>\n<td><a href=\"/wiki/PaLM\" title=\"PaLM\">PaLM 2</a> (Pathways Language Model 2)</td>\n<td><span data-sort-value=\"000000002023-05-01-0000\" style=\"white-space:nowrap\">May 2023</span></td>\n<td>Google</td>\n<td><span data-sort-value=\"340000000000 !\">340</span><sup class=\"reference\" id=\"cite_ref-cnbc-20230516_201-0\"><a href=\"#cite_note-cnbc-20230516-201\">[195]</a></sup></td>\n<td><span data-sort-value=\"3600000000000 !\">3.6 trillion</span> tokens<sup class=\"reference\" id=\"cite_ref-cnbc-20230516_201-1\"><a href=\"#cite_note-cnbc-20230516-201\">[195]</a></sup>\n</td>\n<td>85000<sup class=\"reference\" id=\"cite_ref-:5_187-2\"><a href=\"#cite_note-:5-187\">[183]</a></sup></td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Was used in <a class=\"mw-redirect\" href=\"/wiki/Bard_(chatbot)\" title=\"Bard (chatbot)\">Bard chatbot</a>.<sup class=\"reference\" id=\"cite_ref-pWyLA_202-0\"><a href=\"#cite_note-pWyLA-202\">[196]</a></sup>\n</td></tr>\n<tr>\n<td>Llama 2</td>\n<td><span data-sort-value=\"000000002023-07-01-0000\" style=\"white-space:nowrap\">July 2023</span></td>\n<td>Meta AI</td>\n<td><span data-sort-value=\"70000000000 !\">70</span><sup class=\"reference\" id=\"cite_ref-meta-20230719_203-0\"><a href=\"#cite_note-meta-20230719-203\">[197]</a></sup></td>\n<td><span data-sort-value=\"2000000000000 !\">2 trillion</span> tokens<sup class=\"reference\" id=\"cite_ref-meta-20230719_203-1\"><a href=\"#cite_note-meta-20230719-203\">[197]</a></sup>\n</td>\n<td>21000</td>\n<td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">Llama 2 license\n</td>\n<td>1.7 million A100-hours.<sup class=\"reference\" id=\"cite_ref-204\"><a href=\"#cite_note-204\">[198]</a></sup>\n</td></tr>\n<tr>\n<td><a href=\"/wiki/Claude_(language_model)\" title=\"Claude (language model)\">Claude 2</a>\n</td>\n<td><span data-sort-value=\"000000002023-07-01-0000\" style=\"white-space:nowrap\">July 2023</span>\n</td>\n<td>Anthropic\n</td>\n<td>Unknown\n</td>\n<td>Unknown\n</td>\n<td>Unknown</td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Used in Claude chatbot.<sup class=\"reference\" id=\"cite_ref-205\"><a href=\"#cite_note-205\">[199]</a></sup>\n</td></tr>\n<tr>\n<td>Mistral 7B</td>\n<td><span data-sort-value=\"000000002023-09-01-0000\" style=\"white-space:nowrap\">September 2023</span></td>\n<td><a href=\"/wiki/Mistral_AI\" title=\"Mistral AI\">Mistral AI</a></td>\n<td><span data-sort-value=\"7300000000 !\">7.3</span><sup class=\"reference\" id=\"cite_ref-mistral-20230927_206-0\"><a href=\"#cite_note-mistral-20230927-206\">[200]</a></sup></td>\n<td>Unknown\n</td>\n<td></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n</td>\n<td>\n</td></tr>\n<tr>\n<td><a href=\"/wiki/Claude_(language_model)\" title=\"Claude (language model)\">Claude 2.1</a>\n</td>\n<td><span data-sort-value=\"000000002023-11-01-0000\" style=\"white-space:nowrap\">November 2023</span>\n</td>\n<td>Anthropic\n</td>\n<td>Unknown\n</td>\n<td>Unknown\n</td>\n<td>Unknown</td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Used in Claude chatbot. Has a context window of 200,000 tokens, or ~500 pages.<sup class=\"reference\" id=\"cite_ref-207\"><a href=\"#cite_note-207\">[201]</a></sup>\n</td></tr>\n<tr>\n<td>Grok-1<sup class=\"reference\" id=\"cite_ref-208\"><a href=\"#cite_note-208\">[202]</a></sup>\n</td>\n<td><span data-sort-value=\"000000002023-11-01-0000\" style=\"white-space:nowrap\">November 2023</span>\n</td>\n<td><a class=\"mw-redirect\" href=\"/wiki/X.AI\" title=\"X.AI\">x.AI</a>\n</td>\n<td>314\n</td>\n<td>Unknown\n</td>\n<td>Unknown</td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n</td>\n<td>Used in <a href=\"/wiki/Grok_(chatbot)\" title=\"Grok (chatbot)\">Grok</a> chatbot. Grok-1 has a context length of 8,192 tokens and has access to X (Twitter).<sup class=\"reference\" id=\"cite_ref-209\"><a href=\"#cite_note-209\">[203]</a></sup>\n</td></tr>\n<tr>\n<td><a href=\"/wiki/Gemini_(language_model)\" title=\"Gemini (language model)\">Gemini 1.0</a>\n</td>\n<td><span data-sort-value=\"000000002023-12-01-0000\" style=\"white-space:nowrap\">December 2023</span>\n</td>\n<td><a href=\"/wiki/Google_DeepMind\" title=\"Google DeepMind\">Google DeepMind</a>\n</td>\n<td>Unknown\n</td>\n<td>Unknown\n</td>\n<td>Unknown</td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Multimodal model, comes in three sizes. Used in <a href=\"/wiki/Gemini_(chatbot)\" title=\"Gemini (chatbot)\">the chatbot of the same name</a>.<sup class=\"reference\" id=\"cite_ref-210\"><a href=\"#cite_note-210\">[204]</a></sup>\n</td></tr>\n<tr>\n<td>Mixtral 8x7B\n</td>\n<td><span data-sort-value=\"000000002023-12-01-0000\" style=\"white-space:nowrap\">December 2023</span>\n</td>\n<td><a href=\"/wiki/Mistral_AI\" title=\"Mistral AI\">Mistral AI</a>\n</td>\n<td>46.7\n</td>\n<td>Unknown\n</td>\n<td>Unknown</td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n</td>\n<td>Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.<sup class=\"reference\" id=\"cite_ref-211\"><a href=\"#cite_note-211\">[205]</a></sup> <a href=\"/wiki/Mixture_of_experts\" title=\"Mixture of experts\">Mixture of experts</a> model, with 12.9 billion parameters activated per token.<sup class=\"reference\" id=\"cite_ref-212\"><a href=\"#cite_note-212\">[206]</a></sup>\n</td></tr>\n<tr>\n<td>Mixtral 8x22B\n</td>\n<td><span data-sort-value=\"000000002024-04-01-0000\" style=\"white-space:nowrap\">April 2024</span>\n</td>\n<td><a href=\"/wiki/Mistral_AI\" title=\"Mistral AI\">Mistral AI</a>\n</td>\n<td>141\n</td>\n<td>Unknown\n</td>\n<td>Unknown</td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n</td>\n<td><sup class=\"reference\" id=\"cite_ref-213\"><a href=\"#cite_note-213\">[207]</a></sup>\n</td></tr>\n<tr>\n<td><a class=\"new\" href=\"/w/index.php?title=Phi_(LLM)&amp;action=edit&amp;redlink=1\" title=\"Phi (LLM) (page does not exist)\">Phi-2</a>\n</td>\n<td><span data-sort-value=\"000000002023-12-01-0000\" style=\"white-space:nowrap\">December 2023</span>\n</td>\n<td>Microsoft\n</td>\n<td>2.7\n</td>\n<td>1.4T tokens\n</td>\n<td>419<sup class=\"reference\" id=\"cite_ref-:9_214-0\"><a href=\"#cite_note-:9-214\">[208]</a></sup></td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">MIT\n</td>\n<td>Trained on real and synthetic \"textbook-quality\" data, for 14 days on 96 A100 GPUs.<sup class=\"reference\" id=\"cite_ref-:9_214-1\"><a href=\"#cite_note-:9-214\">[208]</a></sup>\n</td></tr>\n<tr>\n<td><a href=\"/wiki/Gemini_(language_model)\" title=\"Gemini (language model)\">Gemini 1.5</a>\n</td>\n<td><span data-sort-value=\"000000002024-02-01-0000\" style=\"white-space:nowrap\">February 2024</span>\n</td>\n<td><a href=\"/wiki/Google_DeepMind\" title=\"Google DeepMind\">Google DeepMind</a>\n</td>\n<td>Unknown\n</td>\n<td>Unknown\n</td>\n<td>Unknown</td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Multimodal model, based on a <a href=\"/wiki/Mixture_of_experts\" title=\"Mixture of experts\">Mixture-of-Experts</a> (MoE) architecture. Context window above 1 million tokens.<sup class=\"reference\" id=\"cite_ref-215\"><a href=\"#cite_note-215\">[209]</a></sup>\n</td></tr>\n<tr>\n<td>Gemma</td>\n<td><span data-sort-value=\"000000002024-02-01-0000\" style=\"white-space:nowrap\">February 2024</span></td>\n<td><a href=\"/wiki/Google_DeepMind\" title=\"Google DeepMind\">Google DeepMind</a></td>\n<td>7</td>\n<td>6T tokens</td>\n<td>Unknown</td>\n<td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">Gemma Terms of Use<sup class=\"reference\" id=\"cite_ref-gemma_216-0\"><a href=\"#cite_note-gemma-216\">[210]</a></sup></td>\n<td>\n</td></tr>\n<tr>\n<td><a href=\"/wiki/Claude_(language_model)\" title=\"Claude (language model)\">Claude 3</a>\n</td>\n<td>March 2024\n</td>\n<td>Anthropic\n</td>\n<td>Unknown\n</td>\n<td>Unknown\n</td>\n<td>Unknown\n</td>\n<td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n</td>\n<td>Includes three models, Haiku, Sonnet, and Opus.<sup class=\"reference\" id=\"cite_ref-217\"><a href=\"#cite_note-217\">[211]</a></sup>\n</td></tr>\n<tr>\n<td><a href=\"/wiki/DBRX\" title=\"DBRX\">DBRX</a>\n</td>\n<td>March 2024\n</td>\n<td><a href=\"/wiki/Databricks\" title=\"Databricks\">Databricks</a> and <a class=\"mw-redirect\" href=\"/wiki/Mosaic_ML\" title=\"Mosaic ML\">Mosaic ML</a>\n</td>\n<td><span data-sort-value=\"13600000000 !\">136</span>\n</td>\n<td>12T Tokens\n</td>\n<td>\n</td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Databricks Open Model License\n</td>\n<td>Training cost 10 million USD.\n</td></tr>\n<tr>\n<td>Fugaku-LLM\n</td>\n<td>May 2024\n</td>\n<td><a href=\"/wiki/Fujitsu\" title=\"Fujitsu\">Fujitsu</a>, <a href=\"/wiki/Tokyo_Institute_of_Technology\" title=\"Tokyo Institute of Technology\">Tokyo Institute of Technology</a>, etc.\n</td>\n<td><span data-sort-value=\"1300000000 !\">13</span>\n</td>\n<td>380B Tokens\n</td>\n<td>\n</td>\n<td>\n</td>\n<td>The largest model ever trained on CPU-only, on the <a href=\"/wiki/Fugaku_(supercomputer)\" title=\"Fugaku (supercomputer)\">Fugaku</a>.<sup class=\"reference\" id=\"cite_ref-218\"><a href=\"#cite_note-218\">[212]</a></sup>\n</td></tr>\n<tr>\n<td>Llama 3\n</td>\n<td>April 2024\n</td>\n<td>Meta AI\n</td>\n<td>70\n</td>\n<td>15T tokens\n</td>\n<td>100,000</td>\n<td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">Llama 3 license\n</td>\n<td>400B version yet to be released.<sup class=\"reference\" id=\"cite_ref-219\"><a href=\"#cite_note-219\">[213]</a></sup> 70B version took 6.4 million hours on <a href=\"/wiki/Hopper_(microarchitecture)\" title=\"Hopper (microarchitecture)\">H100</a>-80GB.<sup class=\"reference\" id=\"cite_ref-220\"><a href=\"#cite_note-220\">[214]</a></sup><sup class=\"reference\" id=\"cite_ref-221\"><a href=\"#cite_note-221\">[215]</a></sup>\n</td></tr>\n<tr>\n<td><a class=\"new\" href=\"/w/index.php?title=Phi_(LLM)&amp;action=edit&amp;redlink=1\" title=\"Phi (LLM) (page does not exist)\">Phi-3</a> family (mini, small, medium)\n</td>\n<td><span data-sort-value=\"000000002024-04-01-0000\" style=\"white-space:nowrap\">April 2024</span>\n</td>\n<td>Microsoft\n</td>\n<td>3.8B, 7B, 14B <sup class=\"reference\" id=\"cite_ref-222\"><a href=\"#cite_note-222\">[216]</a></sup>\n</td>\n<td>3.3T, 4.8T, 4.8T Tokens\n</td>\n<td>\n</td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">MIT\n</td>\n<td>Microsoft markets them as \"small language model\".<sup class=\"reference\" id=\"cite_ref-223\"><a href=\"#cite_note-223\">[217]</a></sup>\n</td></tr>\n<tr>\n<td>Qwen2\n</td>\n<td><span data-sort-value=\"000000002024-06-01-0000\" style=\"white-space:nowrap\">June 2024</span>\n</td>\n<td>Alibaba Cloud\n</td>\n<td>0.5b, 1.5b, 7b, 57b, and 72b <sup class=\"reference\" id=\"cite_ref-224\"><a href=\"#cite_note-224\">[218]</a></sup>\n</td>\n<td>3T Tokens\n</td>\n<td>\n</td>\n<td>\n</td>\n<td>\n</td></tr>\n<tr>\n<td>Nemotron-4\n</td>\n<td>June 2024\n</td>\n<td><a href=\"/wiki/Nvidia\" title=\"Nvidia\">Nvidia</a>\n</td>\n<td><span data-sort-value=\"34000000000 !\">340</span>\n</td>\n<td>9T Tokens\n</td>\n<td>200,000\n</td>\n<td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">NVIDIA Open Model License\n</td>\n<td>Trained for 1 epoch. Trained on 6144 H100 GPUs between December 2023 and May 2024.<sup class=\"reference\" id=\"cite_ref-225\"><a href=\"#cite_note-225\">[219]</a></sup><sup class=\"reference\" id=\"cite_ref-226\"><a href=\"#cite_note-226\">[220]</a></sup>\n</td></tr></tbody></table>"},"metadata":{}}]},{"cell_type":"code","source":"# Extracting all table headers ('th' tags)\nTitle = List.find_all(\"th\")\nTitle","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:57:29.091139Z","iopub.execute_input":"2024-07-21T20:57:29.092011Z","iopub.status.idle":"2024-07-21T20:57:29.102519Z","shell.execute_reply.started":"2024-07-21T20:57:29.091973Z","shell.execute_reply":"2024-07-21T20:57:29.101285Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[<th>Name</th>,\n <th>Release date<sup class=\"reference\" id=\"cite_ref-133\"><a href=\"#cite_note-133\">[a]</a></sup></th>,\n <th>Developer</th>,\n <th>Number of parameters (billion) <sup class=\"reference\" id=\"cite_ref-134\"><a href=\"#cite_note-134\">[b]</a></sup></th>,\n <th>Corpus size\n </th>,\n <th>Training cost (petaFLOP-day)</th>,\n <th>License<sup class=\"reference\" id=\"cite_ref-135\"><a href=\"#cite_note-135\">[c]</a></sup></th>,\n <th>Notes\n </th>]"},"metadata":{}}]},{"cell_type":"code","source":"Titles = [i.text.strip() for i in Title]\nTitles","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:57:59.082098Z","iopub.execute_input":"2024-07-21T20:57:59.082503Z","iopub.status.idle":"2024-07-21T20:57:59.090857Z","shell.execute_reply.started":"2024-07-21T20:57:59.082462Z","shell.execute_reply":"2024-07-21T20:57:59.089671Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['Name',\n 'Release date[a]',\n 'Developer',\n 'Number of parameters (billion) [b]',\n 'Corpus size',\n 'Training cost (petaFLOP-day)',\n 'License[c]',\n 'Notes']"},"metadata":{}}]},{"cell_type":"code","source":"data = pd.DataFrame(columns=Titles)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:58:11.780997Z","iopub.execute_input":"2024-07-21T20:58:11.781409Z","iopub.status.idle":"2024-07-21T20:58:11.810011Z","shell.execute_reply.started":"2024-07-21T20:58:11.781377Z","shell.execute_reply":"2024-07-21T20:58:11.808754Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Empty DataFrame\nColumns: [Name, Release date[a], Developer, Number of parameters (billion) [b], Corpus size, Training cost (petaFLOP-day), License[c], Notes]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Release date[a]</th>\n      <th>Developer</th>\n      <th>Number of parameters (billion) [b]</th>\n      <th>Corpus size</th>\n      <th>Training cost (petaFLOP-day)</th>\n      <th>License[c]</th>\n      <th>Notes</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"column_data = List.find_all('tr')","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:58:45.983374Z","iopub.execute_input":"2024-07-21T20:58:45.984356Z","iopub.status.idle":"2024-07-21T20:58:45.991829Z","shell.execute_reply.started":"2024-07-21T20:58:45.984307Z","shell.execute_reply":"2024-07-21T20:58:45.990516Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"column_data","metadata":{"execution":{"iopub.status.busy":"2024-07-21T20:58:51.478343Z","iopub.execute_input":"2024-07-21T20:58:51.478788Z","iopub.status.idle":"2024-07-21T20:58:51.520767Z","shell.execute_reply.started":"2024-07-21T20:58:51.478753Z","shell.execute_reply":"2024-07-21T20:58:51.519642Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[<tr>\n <th>Name</th>\n <th>Release date<sup class=\"reference\" id=\"cite_ref-133\"><a href=\"#cite_note-133\">[a]</a></sup></th>\n <th>Developer</th>\n <th>Number of parameters (billion) <sup class=\"reference\" id=\"cite_ref-134\"><a href=\"#cite_note-134\">[b]</a></sup></th>\n <th>Corpus size\n </th>\n <th>Training cost (petaFLOP-day)</th>\n <th>License<sup class=\"reference\" id=\"cite_ref-135\"><a href=\"#cite_note-135\">[c]</a></sup></th>\n <th>Notes\n </th></tr>,\n <tr>\n <td><a href=\"/wiki/GPT-1\" title=\"GPT-1\">GPT-1</a></td>\n <td><span data-sort-value=\"000000002018-06-01-0000\" style=\"white-space:nowrap\">June 2018</span></td>\n <td><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></td>\n <td><span data-sort-value=\"117000000 !\">0.117</span></td>\n <td>\n </td>\n <td>1<sup class=\"reference\" id=\"cite_ref-oai-unsup_136-0\"><a href=\"#cite_note-oai-unsup-136\">[133]</a></sup></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">MIT<sup class=\"reference\" id=\"cite_ref-gpt1_137-0\"><a href=\"#cite_note-gpt1-137\">[134]</a></sup>\n </td>\n <td>First GPT model, decoder-only transformer. Trained for 30 days on 8 P600 <a href=\"/wiki/Graphics_processing_unit\" title=\"Graphics processing unit\">GPUs</a>.\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/BERT_(language_model)\" title=\"BERT (language model)\">BERT</a></td>\n <td><span data-sort-value=\"000000002018-10-01-0000\" style=\"white-space:nowrap\">October 2018</span></td>\n <td><a href=\"/wiki/Google\" title=\"Google\">Google</a></td>\n <td><span data-sort-value=\"340000000 !\">0.340</span><sup class=\"reference\" id=\"cite_ref-bert-paper_138-0\"><a href=\"#cite_note-bert-paper-138\">[135]</a></sup></td>\n <td><span data-sort-value=\"3300000000 !\">3.3 billion</span> words<sup class=\"reference\" id=\"cite_ref-bert-paper_138-1\"><a href=\"#cite_note-bert-paper-138\">[135]</a></sup>\n </td>\n <td><span data-sort-value=\"9 !\">9</span><sup class=\"reference\" id=\"cite_ref-bHZJ2_139-0\"><a href=\"#cite_note-bHZJ2-139\">[136]</a></sup></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0<sup class=\"reference\" id=\"cite_ref-bert-web_140-0\"><a href=\"#cite_note-bert-web-140\">[137]</a></sup>\n </td>\n <td>An early and influential language model,<sup class=\"reference\" id=\"cite_ref-Manning-2022_5-1\"><a href=\"#cite_note-Manning-2022-5\">[5]</a></sup> but encoder-only and thus not built to be prompted or generative<sup class=\"reference\" id=\"cite_ref-Ir545_141-0\"><a href=\"#cite_note-Ir545-141\">[138]</a></sup>\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/T5_(language_model)\" title=\"T5 (language model)\">T5</a>\n </td>\n <td><span data-sort-value=\"000000002019-10-01-0000\" style=\"white-space:nowrap\">October 2019</span>\n </td>\n <td>Google\n </td>\n <td>11<sup class=\"reference\" id=\"cite_ref-:6_142-0\"><a href=\"#cite_note-:6-142\">[139]</a></sup>\n </td>\n <td>34 billion tokens<sup class=\"reference\" id=\"cite_ref-:6_142-1\"><a href=\"#cite_note-:6-142\">[139]</a></sup>\n </td>\n <td>\n </td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0<sup class=\"reference\" id=\"cite_ref-143\"><a href=\"#cite_note-143\">[140]</a></sup>\n </td>\n <td>Base model for many Google projects, such as Imagen.<sup class=\"reference\" id=\"cite_ref-144\"><a href=\"#cite_note-144\">[141]</a></sup>\n </td></tr>,\n <tr>\n <td>XLNet</td>\n <td><span data-sort-value=\"000000002019-06-01-0000\" style=\"white-space:nowrap\">June 2019</span></td>\n <td><a href=\"/wiki/Google\" title=\"Google\">Google</a></td>\n <td><span data-sort-value=\"340000000 !\">~0.340</span><sup class=\"reference\" id=\"cite_ref-45rAm_145-0\"><a href=\"#cite_note-45rAm-145\">[142]</a></sup></td>\n <td><span data-sort-value=\"3300000000 !\">33</span> billion words\n </td>\n <td></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0<sup class=\"reference\" id=\"cite_ref-xlnet_146-0\"><a href=\"#cite_note-xlnet-146\">[143]</a></sup>\n </td>\n <td>An alternative to BERT; designed as encoder-only<sup class=\"reference\" id=\"cite_ref-gAbNO_147-0\"><a href=\"#cite_note-gAbNO-147\">[144]</a></sup><sup class=\"reference\" id=\"cite_ref-LX3rI_148-0\"><a href=\"#cite_note-LX3rI-148\">[145]</a></sup>\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/GPT-2\" title=\"GPT-2\">GPT-2</a></td>\n <td><span data-sort-value=\"000000002019-02-01-0000\" style=\"white-space:nowrap\">February 2019</span></td>\n <td><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></td>\n <td><span data-sort-value=\"1500000000 !\">1.5</span><sup class=\"reference\" id=\"cite_ref-15Brelease_149-0\"><a href=\"#cite_note-15Brelease-149\">[146]</a></sup></td>\n <td>40GB<sup class=\"reference\" id=\"cite_ref-5T8u5_150-0\"><a href=\"#cite_note-5T8u5-150\">[147]</a></sup> (~<span data-sort-value=\"10000000000 !\">10 billion</span> tokens)<sup class=\"reference\" id=\"cite_ref-LambdaLabs_151-0\"><a href=\"#cite_note-LambdaLabs-151\">[148]</a></sup>\n </td>\n <td></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">MIT<sup class=\"reference\" id=\"cite_ref-Sudbe_152-0\"><a href=\"#cite_note-Sudbe-152\">[149]</a></sup>\n </td>\n <td>general-purpose model based on transformer architecture\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/GPT-3\" title=\"GPT-3\">GPT-3</a></td>\n <td><span data-sort-value=\"000000002020-05-01-0000\" style=\"white-space:nowrap\">May 2020</span></td>\n <td>OpenAI</td>\n <td><span data-sort-value=\"175000000000 !\">175</span><sup class=\"reference\" id=\"cite_ref-Wiggers_46-1\"><a href=\"#cite_note-Wiggers-46\">[46]</a></sup></td>\n <td><span data-sort-value=\"300000000000 !\">300 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-LambdaLabs_151-1\"><a href=\"#cite_note-LambdaLabs-151\">[148]</a></sup>\n </td>\n <td>3640<sup class=\"reference\" id=\"cite_ref-:2_153-0\"><a href=\"#cite_note-:2-153\">[150]</a></sup></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">proprietary\n </td>\n <td>A fine-tuned variant of GPT-3, termed GPT-3.5, was made available to the public through a web interface called <a href=\"/wiki/ChatGPT\" title=\"ChatGPT\">ChatGPT</a> in 2022.<sup class=\"reference\" id=\"cite_ref-chatgpt-blog_154-0\"><a href=\"#cite_note-chatgpt-blog-154\">[151]</a></sup>\n </td></tr>,\n <tr>\n <td>GPT-Neo</td>\n <td><span data-sort-value=\"000000002021-03-01-0000\" style=\"white-space:nowrap\">March 2021</span></td>\n <td><a href=\"/wiki/EleutherAI\" title=\"EleutherAI\">EleutherAI</a></td>\n <td><span data-sort-value=\"2700000000 !\">2.7</span><sup class=\"reference\" id=\"cite_ref-gpt-neo_155-0\"><a href=\"#cite_note-gpt-neo-155\">[152]</a></sup></td>\n <td>825 GiB<sup class=\"reference\" id=\"cite_ref-Pile_156-0\"><a href=\"#cite_note-Pile-156\">[153]</a></sup>\n </td>\n <td></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">MIT<sup class=\"reference\" id=\"cite_ref-vb-gpt-neo_157-0\"><a href=\"#cite_note-vb-gpt-neo-157\">[154]</a></sup>\n </td>\n <td>The first of <a href=\"/wiki/EleutherAI#GPT_models\" title=\"EleutherAI\">a series of free GPT-3 alternatives</a> released by EleutherAI. GPT-Neo outperformed an equivalent-size GPT-3 model on some benchmarks, but was significantly worse than the largest GPT-3.<sup class=\"reference\" id=\"cite_ref-vb-gpt-neo_157-1\"><a href=\"#cite_note-vb-gpt-neo-157\">[154]</a></sup>\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/GPT-J\" title=\"GPT-J\">GPT-J</a></td>\n <td><span data-sort-value=\"000000002021-06-01-0000\" style=\"white-space:nowrap\">June 2021</span></td>\n <td><a href=\"/wiki/EleutherAI\" title=\"EleutherAI\">EleutherAI</a></td>\n <td><span data-sort-value=\"6000000000 !\">6</span><sup class=\"reference\" id=\"cite_ref-JxohJ_158-0\"><a href=\"#cite_note-JxohJ-158\">[155]</a></sup></td>\n <td>825 GiB<sup class=\"reference\" id=\"cite_ref-Pile_156-1\"><a href=\"#cite_note-Pile-156\">[153]</a></sup>\n </td>\n <td>200<sup class=\"reference\" id=\"cite_ref-:3_159-0\"><a href=\"#cite_note-:3-159\">[156]</a></sup></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n </td>\n <td>GPT-3-style language model\n </td></tr>,\n <tr>\n <td>Megatron-Turing NLG</td>\n <td><span data-sort-value=\"000000002021-10-01-0000\" style=\"white-space:nowrap\">October 2021</span><sup class=\"reference\" id=\"cite_ref-BwnW5_160-0\"><a href=\"#cite_note-BwnW5-160\">[157]</a></sup></td>\n <td><a href=\"/wiki/Microsoft\" title=\"Microsoft\">Microsoft</a> and <a href=\"/wiki/Nvidia\" title=\"Nvidia\">Nvidia</a></td>\n <td><span data-sort-value=\"530000000000 !\">530</span><sup class=\"reference\" id=\"cite_ref-mtnlg-preprint_161-0\"><a href=\"#cite_note-mtnlg-preprint-161\">[158]</a></sup></td>\n <td><span data-sort-value=\"338600000000 !\">338.6 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-mtnlg-preprint_161-1\"><a href=\"#cite_note-mtnlg-preprint-161\">[158]</a></sup>\n </td>\n <td></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Restricted web access\n </td>\n <td>Standard architecture but trained on a supercomputing cluster.\n </td></tr>,\n <tr>\n <td>Ernie 3.0 Titan</td>\n <td><span data-sort-value=\"000000002021-12-01-0000\" style=\"white-space:nowrap\">December 2021</span></td>\n <td><a href=\"/wiki/Baidu\" title=\"Baidu\">Baidu</a></td>\n <td><span data-sort-value=\"260000000000 !\">260</span><sup class=\"reference\" id=\"cite_ref-qeOB8_162-0\"><a href=\"#cite_note-qeOB8-162\">[159]</a></sup></td>\n <td>4 Tb\n </td>\n <td></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Chinese-language LLM. <a href=\"/wiki/Ernie_Bot\" title=\"Ernie Bot\">Ernie Bot</a> is based on this model.\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/Claude_(language_model)\" title=\"Claude (language model)\">Claude</a><sup class=\"reference\" id=\"cite_ref-i8jc4_163-0\"><a href=\"#cite_note-i8jc4-163\">[160]</a></sup></td>\n <td><span data-sort-value=\"000000002021-12-01-0000\" style=\"white-space:nowrap\">December 2021</span></td>\n <td><a href=\"/wiki/Anthropic\" title=\"Anthropic\">Anthropic</a></td>\n <td><span data-sort-value=\"52000000000 !\">52</span><sup class=\"reference\" id=\"cite_ref-AnthroArch_164-0\"><a href=\"#cite_note-AnthroArch-164\">[161]</a></sup></td>\n <td><span data-sort-value=\"400000000000 !\">400 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-AnthroArch_164-1\"><a href=\"#cite_note-AnthroArch-164\">[161]</a></sup>\n </td>\n <td></td>\n <td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">beta\n </td>\n <td>Fine-tuned for desirable behavior in conversations.<sup class=\"reference\" id=\"cite_ref-RZqhw_165-0\"><a href=\"#cite_note-RZqhw-165\">[162]</a></sup>\n </td></tr>,\n <tr>\n <td>GLaM (Generalist Language Model)</td>\n <td><span data-sort-value=\"000000002021-12-01-0000\" style=\"white-space:nowrap\">December 2021</span></td>\n <td>Google</td>\n <td><span data-sort-value=\"1200000000000 !\">1200</span><sup class=\"reference\" id=\"cite_ref-glam-blog_37-1\"><a href=\"#cite_note-glam-blog-37\">[37]</a></sup></td>\n <td><span data-sort-value=\"1600000000000 !\">1.6 trillion</span> tokens<sup class=\"reference\" id=\"cite_ref-glam-blog_37-2\"><a href=\"#cite_note-glam-blog-37\">[37]</a></sup>\n </td>\n <td>5600<sup class=\"reference\" id=\"cite_ref-glam-blog_37-3\"><a href=\"#cite_note-glam-blog-37\">[37]</a></sup></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Sparse <a href=\"/wiki/Mixture_of_experts\" title=\"Mixture of experts\">mixture of experts</a> model, making it more expensive to train but cheaper to run inference compared to GPT-3.\n </td></tr>,\n <tr>\n <td>Gopher</td>\n <td><span data-sort-value=\"000000002021-12-01-0000\" style=\"white-space:nowrap\">December 2021</span></td>\n <td><a class=\"mw-redirect\" href=\"/wiki/DeepMind\" title=\"DeepMind\">DeepMind</a></td>\n <td><span data-sort-value=\"280000000000 !\">280</span><sup class=\"reference\" id=\"cite_ref-mD5eE_166-0\"><a href=\"#cite_note-mD5eE-166\">[163]</a></sup></td>\n <td><span data-sort-value=\"300000000000 !\">300 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-hoffman_167-0\"><a href=\"#cite_note-hoffman-167\">[164]</a></sup>\n </td>\n <td>5833<sup class=\"reference\" id=\"cite_ref-:4_168-0\"><a href=\"#cite_note-:4-168\">[165]</a></sup></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Later developed into the Chinchilla model.\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/LaMDA\" title=\"LaMDA\">LaMDA</a> (Language Models for Dialog Applications)</td>\n <td><span data-sort-value=\"000000002022-01-01-0000\" style=\"white-space:nowrap\">January 2022</span></td>\n <td>Google</td>\n <td><span data-sort-value=\"137000000000 !\">137</span><sup class=\"reference\" id=\"cite_ref-lamda-blog_169-0\"><a href=\"#cite_note-lamda-blog-169\">[166]</a></sup></td>\n <td>1.56T words,<sup class=\"reference\" id=\"cite_ref-lamda-blog_169-1\"><a href=\"#cite_note-lamda-blog-169\">[166]</a></sup> <span data-sort-value=\"168000000000 !\">168 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-hoffman_167-1\"><a href=\"#cite_note-hoffman-167\">[164]</a></sup>\n </td>\n <td>4110<sup class=\"reference\" id=\"cite_ref-DMs9Z_170-0\"><a href=\"#cite_note-DMs9Z-170\">[167]</a></sup></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Specialized for response generation in conversations.\n </td></tr>,\n <tr>\n <td>GPT-NeoX</td>\n <td><span data-sort-value=\"000000002022-02-01-0000\" style=\"white-space:nowrap\">February 2022</span></td>\n <td><a href=\"/wiki/EleutherAI\" title=\"EleutherAI\">EleutherAI</a></td>\n <td><span data-sort-value=\"20000000000 !\">20</span><sup class=\"reference\" id=\"cite_ref-gpt-neox-20b_171-0\"><a href=\"#cite_note-gpt-neox-20b-171\">[168]</a></sup></td>\n <td>825 GiB<sup class=\"reference\" id=\"cite_ref-Pile_156-2\"><a href=\"#cite_note-Pile-156\">[153]</a></sup>\n </td>\n <td>740<sup class=\"reference\" id=\"cite_ref-:3_159-1\"><a href=\"#cite_note-:3-159\">[156]</a></sup></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n </td>\n <td>based on the Megatron architecture\n </td></tr>,\n <tr>\n <td><a class=\"mw-redirect\" href=\"/wiki/Chinchilla_AI\" title=\"Chinchilla AI\">Chinchilla</a></td>\n <td><span data-sort-value=\"000000002022-03-01-0000\" style=\"white-space:nowrap\">March 2022</span></td>\n <td><a class=\"mw-redirect\" href=\"/wiki/DeepMind\" title=\"DeepMind\">DeepMind</a></td>\n <td><span data-sort-value=\"70000000000 !\">70</span><sup class=\"reference\" id=\"cite_ref-chinchilla-blog_172-0\"><a href=\"#cite_note-chinchilla-blog-172\">[169]</a></sup></td>\n <td><span data-sort-value=\"1400000000000 !\">1.4 trillion</span> tokens<sup class=\"reference\" id=\"cite_ref-chinchilla-blog_172-1\"><a href=\"#cite_note-chinchilla-blog-172\">[169]</a></sup><sup class=\"reference\" id=\"cite_ref-hoffman_167-2\"><a href=\"#cite_note-hoffman-167\">[164]</a></sup>\n </td>\n <td>6805<sup class=\"reference\" id=\"cite_ref-:4_168-1\"><a href=\"#cite_note-:4-168\">[165]</a></sup></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Reduced-parameter model trained on more data. Used in the <a class=\"mw-redirect\" href=\"/wiki/Sparrow_(bot)\" title=\"Sparrow (bot)\">Sparrow</a> bot. Often cited for its <a href=\"/wiki/Neural_scaling_law\" title=\"Neural scaling law\">neural scaling law</a>.\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/PaLM\" title=\"PaLM\">PaLM</a> (Pathways Language Model)</td>\n <td><span data-sort-value=\"000000002022-04-01-0000\" style=\"white-space:nowrap\">April 2022</span></td>\n <td>Google</td>\n <td><span data-sort-value=\"540000000000 !\">540</span><sup class=\"reference\" id=\"cite_ref-palm-blog_173-0\"><a href=\"#cite_note-palm-blog-173\">[170]</a></sup></td>\n <td><span data-sort-value=\"768000000000 !\">768 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-chinchilla-blog_172-2\"><a href=\"#cite_note-chinchilla-blog-172\">[169]</a></sup>\n </td>\n <td>29250<sup class=\"reference\" id=\"cite_ref-:4_168-2\"><a href=\"#cite_note-:4-168\">[165]</a></sup></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Trained for ~60 days on ~6000 <a href=\"/wiki/Tensor_Processing_Unit\" title=\"Tensor Processing Unit\">TPU v4</a> chips.<sup class=\"reference\" id=\"cite_ref-:4_168-3\"><a href=\"#cite_note-:4-168\">[165]</a></sup>\n </td></tr>,\n <tr>\n <td>OPT (Open Pretrained Transformer)</td>\n <td><span data-sort-value=\"000000002022-05-01-0000\" style=\"white-space:nowrap\">May 2022</span></td>\n <td><a href=\"/wiki/Meta_Platforms\" title=\"Meta Platforms\">Meta</a></td>\n <td><span data-sort-value=\"175000000000 !\">175</span><sup class=\"reference\" id=\"cite_ref-jlof8_174-0\"><a href=\"#cite_note-jlof8-174\">[171]</a></sup></td>\n <td><span data-sort-value=\"180000000000 !\">180 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-QjTIc_175-0\"><a href=\"#cite_note-QjTIc-175\">[172]</a></sup>\n </td>\n <td>310<sup class=\"reference\" id=\"cite_ref-:3_159-2\"><a href=\"#cite_note-:3-159\">[156]</a></sup></td>\n <td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">Non-commercial research<sup class=\"reference\" id=\"cite_ref-176\"><a href=\"#cite_note-176\">[d]</a></sup>\n </td>\n <td>GPT-3 architecture with some adaptations from Megatron\n </td></tr>,\n <tr>\n <td>YaLM 100B</td>\n <td><span data-sort-value=\"000000002022-06-01-0000\" style=\"white-space:nowrap\">June 2022</span></td>\n <td><a href=\"/wiki/Yandex\" title=\"Yandex\">Yandex</a></td>\n <td><span data-sort-value=\"100000000000 !\">100</span><sup class=\"reference\" id=\"cite_ref-yalm-repo_177-0\"><a href=\"#cite_note-yalm-repo-177\">[173]</a></sup>\n </td>\n <td>1.7TB<sup class=\"reference\" id=\"cite_ref-yalm-repo_177-1\"><a href=\"#cite_note-yalm-repo-177\">[173]</a></sup></td>\n <td></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0</td>\n <td>English-Russian model based on Microsoft's Megatron-LM.\n </td></tr>,\n <tr>\n <td>Minerva</td>\n <td><span data-sort-value=\"000000002022-06-01-0000\" style=\"white-space:nowrap\">June 2022</span></td>\n <td>Google</td>\n <td><span data-sort-value=\"540000000000 !\">540</span><sup class=\"reference\" id=\"cite_ref-minerva-paper_178-0\"><a href=\"#cite_note-minerva-paper-178\">[174]</a></sup></td>\n <td>38.5B tokens from webpages filtered for mathematical content and from papers submitted to the arXiv preprint server<sup class=\"reference\" id=\"cite_ref-minerva-paper_178-1\"><a href=\"#cite_note-minerva-paper-178\">[174]</a></sup>\n </td>\n <td></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>For solving \"mathematical and scientific questions using step-by-step reasoning\".<sup class=\"reference\" id=\"cite_ref-FfCNK_179-0\"><a href=\"#cite_note-FfCNK-179\">[175]</a></sup> Based on PaLM model, further trained on mathematical and scientific data.\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/BLOOM_(language_model)\" title=\"BLOOM (language model)\">BLOOM</a></td>\n <td><span data-sort-value=\"000000002022-07-01-0000\" style=\"white-space:nowrap\">July 2022</span></td>\n <td>Large collaboration led by <a href=\"/wiki/Hugging_Face\" title=\"Hugging Face\">Hugging Face</a></td>\n <td><span data-sort-value=\"175000000000 !\">175</span><sup class=\"reference\" id=\"cite_ref-bigger-better_180-0\"><a href=\"#cite_note-bigger-better-180\">[176]</a></sup></td>\n <td><span data-sort-value=\"350000000000 !\">350 billion</span> tokens (1.6TB)<sup class=\"reference\" id=\"cite_ref-B8wB2_181-0\"><a href=\"#cite_note-B8wB2-181\">[177]</a></sup>\n </td>\n <td></td>\n <td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">Responsible AI\n </td>\n <td>Essentially GPT-3 but trained on a multi-lingual corpus (30% English excluding programming languages)\n </td></tr>,\n <tr>\n <td>Galactica</td>\n <td><span data-sort-value=\"000000002022-11-01-0000\" style=\"white-space:nowrap\">November 2022</span></td>\n <td><a href=\"/wiki/Meta_Platforms\" title=\"Meta Platforms\">Meta</a></td>\n <td><span data-sort-value=\"120000000000 !\">120</span></td>\n <td><span data-sort-value=\"350000000000 !\">106 billion</span> tokens<sup class=\"reference\" id=\"cite_ref-37sY6_182-0\"><a href=\"#cite_note-37sY6-182\">[178]</a></sup>\n </td>\n <td>unknown</td>\n <td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">CC-BY-NC-4.0\n </td>\n <td>Trained on scientific text and modalities.\n </td></tr>,\n <tr>\n <td>AlexaTM (Teacher Models)</td>\n <td><span data-sort-value=\"000000002022-11-01-0000\" style=\"white-space:nowrap\">November 2022</span></td>\n <td><a href=\"/wiki/Amazon_(company)\" title=\"Amazon (company)\">Amazon</a></td>\n <td><span data-sort-value=\"20000000000 !\">20</span><sup class=\"reference\" id=\"cite_ref-u5szh_183-0\"><a href=\"#cite_note-u5szh-183\">[179]</a></sup></td>\n <td><span data-sort-value=\"1300000000000 !\">1.3 trillion</span><sup class=\"reference\" id=\"cite_ref-HaA7l_184-0\"><a href=\"#cite_note-HaA7l-184\">[180]</a></sup>\n </td>\n <td></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">proprietary<sup class=\"reference\" id=\"cite_ref-rpehM_185-0\"><a href=\"#cite_note-rpehM-185\">[181]</a></sup>\n </td>\n <td>bidirectional sequence-to-sequence architecture\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/Neuro-sama\" title=\"Neuro-sama\">Neuro-sama</a></td>\n <td><span data-sort-value=\"000000002022-12-01-0000\" style=\"white-space:nowrap\">December 2022</span></td>\n <td>Independent</td>\n <td>Unknown</td>\n <td>Unknown\n </td>\n <td></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">privately-owned\n </td>\n <td>A language model designed for live-streaming on <a href=\"/wiki/Twitch_(service)\" title=\"Twitch (service)\">Twitch</a>.\n </td></tr>,\n <tr>\n <td><a class=\"mw-redirect\" href=\"/wiki/LLaMA\" title=\"LLaMA\">LLaMA</a> (Large Language Model Meta AI)</td>\n <td><span data-sort-value=\"000000002023-02-01-0000\" style=\"white-space:nowrap\">February 2023</span></td>\n <td><a href=\"/wiki/Meta_AI\" title=\"Meta AI\">Meta AI</a></td>\n <td><span data-sort-value=\"65000000000 !\">65</span><sup class=\"reference\" id=\"cite_ref-llama-blog_186-0\"><a href=\"#cite_note-llama-blog-186\">[182]</a></sup></td>\n <td><span data-sort-value=\"1400000000000 !\">1.4 trillion</span><sup class=\"reference\" id=\"cite_ref-llama-blog_186-1\"><a href=\"#cite_note-llama-blog-186\">[182]</a></sup>\n </td>\n <td>6300<sup class=\"reference\" id=\"cite_ref-:5_187-0\"><a href=\"#cite_note-:5-187\">[183]</a></sup></td>\n <td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">Non-commercial research<sup class=\"reference\" id=\"cite_ref-188\"><a href=\"#cite_note-188\">[e]</a></sup>\n </td>\n <td>Corpus has 20 languages. \"Overtrained\" (compared to <a href=\"/wiki/Chinchilla_(language_model)\" title=\"Chinchilla (language model)\">Chinchilla scaling law</a>) for better performance with fewer parameters.<sup class=\"reference\" id=\"cite_ref-llama-blog_186-2\"><a href=\"#cite_note-llama-blog-186\">[182]</a></sup>\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/GPT-4\" title=\"GPT-4\">GPT-4</a></td>\n <td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span></td>\n <td>OpenAI</td>\n <td>Unknown<sup class=\"reference\" id=\"cite_ref-190\"><a href=\"#cite_note-190\">[f]</a></sup></td>\n <td>Unknown\n </td>\n <td>Unknown</td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">proprietary\n </td>\n <td>Available for ChatGPT Plus users and used in <a href=\"/wiki/GPT-4#Usage\" title=\"GPT-4\">several products</a>.\n </td></tr>,\n <tr>\n <td>Cerebras-GPT\n </td>\n <td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span>\n </td>\n <td><a href=\"/wiki/Cerebras\" title=\"Cerebras\">Cerebras</a>\n </td>\n <td><span data-sort-value=\"13000000000 !\">13</span><sup class=\"reference\" id=\"cite_ref-D0k2a_191-0\"><a href=\"#cite_note-D0k2a-191\">[185]</a></sup>\n </td>\n <td>\n </td>\n <td>270<sup class=\"reference\" id=\"cite_ref-:3_159-3\"><a href=\"#cite_note-:3-159\">[156]</a></sup></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n </td>\n <td>Trained with <a href=\"/wiki/Chinchilla_(language_model)\" title=\"Chinchilla (language model)\">Chinchilla formula</a>.\n </td></tr>,\n <tr>\n <td>Falcon</td>\n <td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span></td>\n <td><a href=\"/wiki/Technology_Innovation_Institute\" title=\"Technology Innovation Institute\">Technology Innovation Institute</a></td>\n <td><span data-sort-value=\"40000000000 !\">40</span><sup class=\"reference\" id=\"cite_ref-falcon_192-0\"><a href=\"#cite_note-falcon-192\">[186]</a></sup></td>\n <td>1 trillion tokens, from RefinedWeb (filtered web text corpus)<sup class=\"reference\" id=\"cite_ref-Xb1gq_193-0\"><a href=\"#cite_note-Xb1gq-193\">[187]</a></sup> plus some \"curated corpora\".<sup class=\"reference\" id=\"cite_ref-gzTNw_194-0\"><a href=\"#cite_note-gzTNw-194\">[188]</a></sup>\n </td>\n <td>2800<sup class=\"reference\" id=\"cite_ref-:5_187-1\"><a href=\"#cite_note-:5-187\">[183]</a></sup></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0<sup class=\"reference\" id=\"cite_ref-Wmlcs_195-0\"><a href=\"#cite_note-Wmlcs-195\">[189]</a></sup>\n </td>\n <td>\n </td></tr>,\n <tr>\n <td>BloombergGPT</td>\n <td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span></td>\n <td><a href=\"/wiki/Bloomberg_L.P.\" title=\"Bloomberg L.P.\">Bloomberg L.P.</a></td>\n <td><span data-sort-value=\"50000000000 !\">50</span></td>\n <td>363 billion token dataset based on Bloomberg's data sources, plus 345 billion tokens from general purpose datasets<sup class=\"reference\" id=\"cite_ref-nGOSu_196-0\"><a href=\"#cite_note-nGOSu-196\">[190]</a></sup>\n </td>\n <td></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Trained on financial data from proprietary sources, for financial tasks.\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/Huawei_PanGu\" title=\"Huawei PanGu\">PanGu-Σ</a></td>\n <td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span></td>\n <td><a href=\"/wiki/Huawei\" title=\"Huawei\">Huawei</a></td>\n <td><span data-sort-value=\"1085000000000 !\">1085</span></td>\n <td>329 billion tokens<sup class=\"reference\" id=\"cite_ref-9WSFw_197-0\"><a href=\"#cite_note-9WSFw-197\">[191]</a></sup>\n </td>\n <td></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>\n </td></tr>,\n <tr>\n <td>OpenAssistant<sup class=\"reference\" id=\"cite_ref-JiOl8_198-0\"><a href=\"#cite_note-JiOl8-198\">[192]</a></sup></td>\n <td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span></td>\n <td><a href=\"/wiki/LAION\" title=\"LAION\">LAION</a></td>\n <td><span data-sort-value=\"17000000000 !\">17</span></td>\n <td>1.5 trillion tokens\n </td>\n <td></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n </td>\n <td>Trained on crowdsourced open data\n </td></tr>,\n <tr>\n <td>Jurassic-2<sup class=\"reference\" id=\"cite_ref-199\"><a href=\"#cite_note-199\">[193]</a></sup>\n </td>\n <td><span data-sort-value=\"000000002023-03-01-0000\" style=\"white-space:nowrap\">March 2023</span>\n </td>\n <td><a href=\"/wiki/AI21_Labs\" title=\"AI21 Labs\">AI21 Labs</a>\n </td>\n <td>Unknown\n </td>\n <td>Unknown\n </td>\n <td></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Multilingual<sup class=\"reference\" id=\"cite_ref-200\"><a href=\"#cite_note-200\">[194]</a></sup>\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/PaLM\" title=\"PaLM\">PaLM 2</a> (Pathways Language Model 2)</td>\n <td><span data-sort-value=\"000000002023-05-01-0000\" style=\"white-space:nowrap\">May 2023</span></td>\n <td>Google</td>\n <td><span data-sort-value=\"340000000000 !\">340</span><sup class=\"reference\" id=\"cite_ref-cnbc-20230516_201-0\"><a href=\"#cite_note-cnbc-20230516-201\">[195]</a></sup></td>\n <td><span data-sort-value=\"3600000000000 !\">3.6 trillion</span> tokens<sup class=\"reference\" id=\"cite_ref-cnbc-20230516_201-1\"><a href=\"#cite_note-cnbc-20230516-201\">[195]</a></sup>\n </td>\n <td>85000<sup class=\"reference\" id=\"cite_ref-:5_187-2\"><a href=\"#cite_note-:5-187\">[183]</a></sup></td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Was used in <a class=\"mw-redirect\" href=\"/wiki/Bard_(chatbot)\" title=\"Bard (chatbot)\">Bard chatbot</a>.<sup class=\"reference\" id=\"cite_ref-pWyLA_202-0\"><a href=\"#cite_note-pWyLA-202\">[196]</a></sup>\n </td></tr>,\n <tr>\n <td>Llama 2</td>\n <td><span data-sort-value=\"000000002023-07-01-0000\" style=\"white-space:nowrap\">July 2023</span></td>\n <td>Meta AI</td>\n <td><span data-sort-value=\"70000000000 !\">70</span><sup class=\"reference\" id=\"cite_ref-meta-20230719_203-0\"><a href=\"#cite_note-meta-20230719-203\">[197]</a></sup></td>\n <td><span data-sort-value=\"2000000000000 !\">2 trillion</span> tokens<sup class=\"reference\" id=\"cite_ref-meta-20230719_203-1\"><a href=\"#cite_note-meta-20230719-203\">[197]</a></sup>\n </td>\n <td>21000</td>\n <td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">Llama 2 license\n </td>\n <td>1.7 million A100-hours.<sup class=\"reference\" id=\"cite_ref-204\"><a href=\"#cite_note-204\">[198]</a></sup>\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/Claude_(language_model)\" title=\"Claude (language model)\">Claude 2</a>\n </td>\n <td><span data-sort-value=\"000000002023-07-01-0000\" style=\"white-space:nowrap\">July 2023</span>\n </td>\n <td>Anthropic\n </td>\n <td>Unknown\n </td>\n <td>Unknown\n </td>\n <td>Unknown</td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Used in Claude chatbot.<sup class=\"reference\" id=\"cite_ref-205\"><a href=\"#cite_note-205\">[199]</a></sup>\n </td></tr>,\n <tr>\n <td>Mistral 7B</td>\n <td><span data-sort-value=\"000000002023-09-01-0000\" style=\"white-space:nowrap\">September 2023</span></td>\n <td><a href=\"/wiki/Mistral_AI\" title=\"Mistral AI\">Mistral AI</a></td>\n <td><span data-sort-value=\"7300000000 !\">7.3</span><sup class=\"reference\" id=\"cite_ref-mistral-20230927_206-0\"><a href=\"#cite_note-mistral-20230927-206\">[200]</a></sup></td>\n <td>Unknown\n </td>\n <td></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n </td>\n <td>\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/Claude_(language_model)\" title=\"Claude (language model)\">Claude 2.1</a>\n </td>\n <td><span data-sort-value=\"000000002023-11-01-0000\" style=\"white-space:nowrap\">November 2023</span>\n </td>\n <td>Anthropic\n </td>\n <td>Unknown\n </td>\n <td>Unknown\n </td>\n <td>Unknown</td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Used in Claude chatbot. Has a context window of 200,000 tokens, or ~500 pages.<sup class=\"reference\" id=\"cite_ref-207\"><a href=\"#cite_note-207\">[201]</a></sup>\n </td></tr>,\n <tr>\n <td>Grok-1<sup class=\"reference\" id=\"cite_ref-208\"><a href=\"#cite_note-208\">[202]</a></sup>\n </td>\n <td><span data-sort-value=\"000000002023-11-01-0000\" style=\"white-space:nowrap\">November 2023</span>\n </td>\n <td><a class=\"mw-redirect\" href=\"/wiki/X.AI\" title=\"X.AI\">x.AI</a>\n </td>\n <td>314\n </td>\n <td>Unknown\n </td>\n <td>Unknown</td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n </td>\n <td>Used in <a href=\"/wiki/Grok_(chatbot)\" title=\"Grok (chatbot)\">Grok</a> chatbot. Grok-1 has a context length of 8,192 tokens and has access to X (Twitter).<sup class=\"reference\" id=\"cite_ref-209\"><a href=\"#cite_note-209\">[203]</a></sup>\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/Gemini_(language_model)\" title=\"Gemini (language model)\">Gemini 1.0</a>\n </td>\n <td><span data-sort-value=\"000000002023-12-01-0000\" style=\"white-space:nowrap\">December 2023</span>\n </td>\n <td><a href=\"/wiki/Google_DeepMind\" title=\"Google DeepMind\">Google DeepMind</a>\n </td>\n <td>Unknown\n </td>\n <td>Unknown\n </td>\n <td>Unknown</td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Multimodal model, comes in three sizes. Used in <a href=\"/wiki/Gemini_(chatbot)\" title=\"Gemini (chatbot)\">the chatbot of the same name</a>.<sup class=\"reference\" id=\"cite_ref-210\"><a href=\"#cite_note-210\">[204]</a></sup>\n </td></tr>,\n <tr>\n <td>Mixtral 8x7B\n </td>\n <td><span data-sort-value=\"000000002023-12-01-0000\" style=\"white-space:nowrap\">December 2023</span>\n </td>\n <td><a href=\"/wiki/Mistral_AI\" title=\"Mistral AI\">Mistral AI</a>\n </td>\n <td>46.7\n </td>\n <td>Unknown\n </td>\n <td>Unknown</td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n </td>\n <td>Outperforms GPT-3.5 and Llama 2 70B on many benchmarks.<sup class=\"reference\" id=\"cite_ref-211\"><a href=\"#cite_note-211\">[205]</a></sup> <a href=\"/wiki/Mixture_of_experts\" title=\"Mixture of experts\">Mixture of experts</a> model, with 12.9 billion parameters activated per token.<sup class=\"reference\" id=\"cite_ref-212\"><a href=\"#cite_note-212\">[206]</a></sup>\n </td></tr>,\n <tr>\n <td>Mixtral 8x22B\n </td>\n <td><span data-sort-value=\"000000002024-04-01-0000\" style=\"white-space:nowrap\">April 2024</span>\n </td>\n <td><a href=\"/wiki/Mistral_AI\" title=\"Mistral AI\">Mistral AI</a>\n </td>\n <td>141\n </td>\n <td>Unknown\n </td>\n <td>Unknown</td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Apache 2.0\n </td>\n <td><sup class=\"reference\" id=\"cite_ref-213\"><a href=\"#cite_note-213\">[207]</a></sup>\n </td></tr>,\n <tr>\n <td><a class=\"new\" href=\"/w/index.php?title=Phi_(LLM)&amp;action=edit&amp;redlink=1\" title=\"Phi (LLM) (page does not exist)\">Phi-2</a>\n </td>\n <td><span data-sort-value=\"000000002023-12-01-0000\" style=\"white-space:nowrap\">December 2023</span>\n </td>\n <td>Microsoft\n </td>\n <td>2.7\n </td>\n <td>1.4T tokens\n </td>\n <td>419<sup class=\"reference\" id=\"cite_ref-:9_214-0\"><a href=\"#cite_note-:9-214\">[208]</a></sup></td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">MIT\n </td>\n <td>Trained on real and synthetic \"textbook-quality\" data, for 14 days on 96 A100 GPUs.<sup class=\"reference\" id=\"cite_ref-:9_214-1\"><a href=\"#cite_note-:9-214\">[208]</a></sup>\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/Gemini_(language_model)\" title=\"Gemini (language model)\">Gemini 1.5</a>\n </td>\n <td><span data-sort-value=\"000000002024-02-01-0000\" style=\"white-space:nowrap\">February 2024</span>\n </td>\n <td><a href=\"/wiki/Google_DeepMind\" title=\"Google DeepMind\">Google DeepMind</a>\n </td>\n <td>Unknown\n </td>\n <td>Unknown\n </td>\n <td>Unknown</td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Multimodal model, based on a <a href=\"/wiki/Mixture_of_experts\" title=\"Mixture of experts\">Mixture-of-Experts</a> (MoE) architecture. Context window above 1 million tokens.<sup class=\"reference\" id=\"cite_ref-215\"><a href=\"#cite_note-215\">[209]</a></sup>\n </td></tr>,\n <tr>\n <td>Gemma</td>\n <td><span data-sort-value=\"000000002024-02-01-0000\" style=\"white-space:nowrap\">February 2024</span></td>\n <td><a href=\"/wiki/Google_DeepMind\" title=\"Google DeepMind\">Google DeepMind</a></td>\n <td>7</td>\n <td>6T tokens</td>\n <td>Unknown</td>\n <td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">Gemma Terms of Use<sup class=\"reference\" id=\"cite_ref-gemma_216-0\"><a href=\"#cite_note-gemma-216\">[210]</a></sup></td>\n <td>\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/Claude_(language_model)\" title=\"Claude (language model)\">Claude 3</a>\n </td>\n <td>March 2024\n </td>\n <td>Anthropic\n </td>\n <td>Unknown\n </td>\n <td>Unknown\n </td>\n <td>Unknown\n </td>\n <td class=\"table-no\" style=\"background:#FFC7C7;color:black;vertical-align:middle;text-align:center;\">Proprietary\n </td>\n <td>Includes three models, Haiku, Sonnet, and Opus.<sup class=\"reference\" id=\"cite_ref-217\"><a href=\"#cite_note-217\">[211]</a></sup>\n </td></tr>,\n <tr>\n <td><a href=\"/wiki/DBRX\" title=\"DBRX\">DBRX</a>\n </td>\n <td>March 2024\n </td>\n <td><a href=\"/wiki/Databricks\" title=\"Databricks\">Databricks</a> and <a class=\"mw-redirect\" href=\"/wiki/Mosaic_ML\" title=\"Mosaic ML\">Mosaic ML</a>\n </td>\n <td><span data-sort-value=\"13600000000 !\">136</span>\n </td>\n <td>12T Tokens\n </td>\n <td>\n </td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">Databricks Open Model License\n </td>\n <td>Training cost 10 million USD.\n </td></tr>,\n <tr>\n <td>Fugaku-LLM\n </td>\n <td>May 2024\n </td>\n <td><a href=\"/wiki/Fujitsu\" title=\"Fujitsu\">Fujitsu</a>, <a href=\"/wiki/Tokyo_Institute_of_Technology\" title=\"Tokyo Institute of Technology\">Tokyo Institute of Technology</a>, etc.\n </td>\n <td><span data-sort-value=\"1300000000 !\">13</span>\n </td>\n <td>380B Tokens\n </td>\n <td>\n </td>\n <td>\n </td>\n <td>The largest model ever trained on CPU-only, on the <a href=\"/wiki/Fugaku_(supercomputer)\" title=\"Fugaku (supercomputer)\">Fugaku</a>.<sup class=\"reference\" id=\"cite_ref-218\"><a href=\"#cite_note-218\">[212]</a></sup>\n </td></tr>,\n <tr>\n <td>Llama 3\n </td>\n <td>April 2024\n </td>\n <td>Meta AI\n </td>\n <td>70\n </td>\n <td>15T tokens\n </td>\n <td>100,000</td>\n <td class=\"table-partial\" style=\"background: #FFB; color:black; vertical-align: middle; text-align: center;\">Llama 3 license\n </td>\n <td>400B version yet to be released.<sup class=\"reference\" id=\"cite_ref-219\"><a href=\"#cite_note-219\">[213]</a></sup> 70B version took 6.4 million hours on <a href=\"/wiki/Hopper_(microarchitecture)\" title=\"Hopper (microarchitecture)\">H100</a>-80GB.<sup class=\"reference\" id=\"cite_ref-220\"><a href=\"#cite_note-220\">[214]</a></sup><sup class=\"reference\" id=\"cite_ref-221\"><a href=\"#cite_note-221\">[215]</a></sup>\n </td></tr>,\n <tr>\n <td><a class=\"new\" href=\"/w/index.php?title=Phi_(LLM)&amp;action=edit&amp;redlink=1\" title=\"Phi (LLM) (page does not exist)\">Phi-3</a> family (mini, small, medium)\n </td>\n <td><span data-sort-value=\"000000002024-04-01-0000\" style=\"white-space:nowrap\">April 2024</span>\n </td>\n <td>Microsoft\n </td>\n <td>3.8B, 7B, 14B <sup class=\"reference\" id=\"cite_ref-222\"><a href=\"#cite_note-222\">[216]</a></sup>\n </td>\n <td>3.3T, 4.8T, 4.8T Tokens\n </td>\n <td>\n </td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">MIT\n </td>\n <td>Microsoft markets them as \"small language model\".<sup class=\"reference\" id=\"cite_ref-223\"><a href=\"#cite_note-223\">[217]</a></sup>\n </td></tr>,\n <tr>\n <td>Qwen2\n </td>\n <td><span data-sort-value=\"000000002024-06-01-0000\" style=\"white-space:nowrap\">June 2024</span>\n </td>\n <td>Alibaba Cloud\n </td>\n <td>0.5b, 1.5b, 7b, 57b, and 72b <sup class=\"reference\" id=\"cite_ref-224\"><a href=\"#cite_note-224\">[218]</a></sup>\n </td>\n <td>3T Tokens\n </td>\n <td>\n </td>\n <td>\n </td>\n <td>\n </td></tr>,\n <tr>\n <td>Nemotron-4\n </td>\n <td>June 2024\n </td>\n <td><a href=\"/wiki/Nvidia\" title=\"Nvidia\">Nvidia</a>\n </td>\n <td><span data-sort-value=\"34000000000 !\">340</span>\n </td>\n <td>9T Tokens\n </td>\n <td>200,000\n </td>\n <td class=\"table-yes\" style=\"background:#9EFF9E;color:black;vertical-align:middle;text-align:center;\">NVIDIA Open Model License\n </td>\n <td>Trained for 1 epoch. Trained on 6144 H100 GPUs between December 2023 and May 2024.<sup class=\"reference\" id=\"cite_ref-225\"><a href=\"#cite_note-225\">[219]</a></sup><sup class=\"reference\" id=\"cite_ref-226\"><a href=\"#cite_note-226\">[220]</a></sup>\n </td></tr>]"},"metadata":{}}]},{"cell_type":"code","source":"for i in column_data[1:]:\n    row_data = i.find_all('td')\n    row = [tr.text.strip() for tr in row_data]\n    length = len(data)\n    data.loc[length] = row","metadata":{"execution":{"iopub.status.busy":"2024-07-21T21:01:54.067936Z","iopub.execute_input":"2024-07-21T21:01:54.068382Z","iopub.status.idle":"2024-07-21T21:01:54.159225Z","shell.execute_reply.started":"2024-07-21T21:01:54.068346Z","shell.execute_reply":"2024-07-21T21:01:54.157767Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-07-21T21:02:04.820317Z","iopub.execute_input":"2024-07-21T21:02:04.821659Z","iopub.status.idle":"2024-07-21T21:02:04.855692Z","shell.execute_reply.started":"2024-07-21T21:02:04.821612Z","shell.execute_reply":"2024-07-21T21:02:04.854086Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                               Name    Release date[a]  \\\n0                                             GPT-1          June 2018   \n1                                              BERT       October 2018   \n2                                                T5       October 2019   \n3                                             XLNet          June 2019   \n4                                             GPT-2      February 2019   \n5                                             GPT-3           May 2020   \n6                                           GPT-Neo         March 2021   \n7                                             GPT-J          June 2021   \n8                               Megatron-Turing NLG  October 2021[157]   \n9                                   Ernie 3.0 Titan      December 2021   \n10                                      Claude[160]      December 2021   \n11                 GLaM (Generalist Language Model)      December 2021   \n12                                           Gopher      December 2021   \n13  LaMDA (Language Models for Dialog Applications)       January 2022   \n14                                         GPT-NeoX      February 2022   \n15                                       Chinchilla         March 2022   \n16                   PaLM (Pathways Language Model)         April 2022   \n17                OPT (Open Pretrained Transformer)           May 2022   \n18                                        YaLM 100B          June 2022   \n19                                          Minerva          June 2022   \n20                                            BLOOM          July 2022   \n21                                        Galactica      November 2022   \n22                         AlexaTM (Teacher Models)      November 2022   \n23                                       Neuro-sama      December 2022   \n24             LLaMA (Large Language Model Meta AI)      February 2023   \n25                                            GPT-4         March 2023   \n26                                     Cerebras-GPT         March 2023   \n27                                           Falcon         March 2023   \n28                                     BloombergGPT         March 2023   \n29                                          PanGu-Σ         March 2023   \n30                               OpenAssistant[192]         March 2023   \n31                                  Jurassic-2[193]         March 2023   \n32               PaLM 2 (Pathways Language Model 2)           May 2023   \n33                                          Llama 2          July 2023   \n34                                         Claude 2          July 2023   \n35                                       Mistral 7B     September 2023   \n36                                       Claude 2.1      November 2023   \n37                                      Grok-1[202]      November 2023   \n38                                       Gemini 1.0      December 2023   \n39                                     Mixtral 8x7B      December 2023   \n40                                    Mixtral 8x22B         April 2024   \n41                                            Phi-2      December 2023   \n42                                       Gemini 1.5      February 2024   \n43                                            Gemma      February 2024   \n44                                         Claude 3         March 2024   \n45                                             DBRX         March 2024   \n46                                       Fugaku-LLM           May 2024   \n47                                          Llama 3         April 2024   \n48               Phi-3 family (mini, small, medium)         April 2024   \n49                                            Qwen2          June 2024   \n50                                       Nemotron-4          June 2024   \n\n                                       Developer  \\\n0                                         OpenAI   \n1                                         Google   \n2                                         Google   \n3                                         Google   \n4                                         OpenAI   \n5                                         OpenAI   \n6                                     EleutherAI   \n7                                     EleutherAI   \n8                           Microsoft and Nvidia   \n9                                          Baidu   \n10                                     Anthropic   \n11                                        Google   \n12                                      DeepMind   \n13                                        Google   \n14                                    EleutherAI   \n15                                      DeepMind   \n16                                        Google   \n17                                          Meta   \n18                                        Yandex   \n19                                        Google   \n20       Large collaboration led by Hugging Face   \n21                                          Meta   \n22                                        Amazon   \n23                                   Independent   \n24                                       Meta AI   \n25                                        OpenAI   \n26                                      Cerebras   \n27               Technology Innovation Institute   \n28                                Bloomberg L.P.   \n29                                        Huawei   \n30                                         LAION   \n31                                     AI21 Labs   \n32                                        Google   \n33                                       Meta AI   \n34                                     Anthropic   \n35                                    Mistral AI   \n36                                     Anthropic   \n37                                          x.AI   \n38                               Google DeepMind   \n39                                    Mistral AI   \n40                                    Mistral AI   \n41                                     Microsoft   \n42                               Google DeepMind   \n43                               Google DeepMind   \n44                                     Anthropic   \n45                      Databricks and Mosaic ML   \n46  Fujitsu, Tokyo Institute of Technology, etc.   \n47                                       Meta AI   \n48                                     Microsoft   \n49                                 Alibaba Cloud   \n50                                        Nvidia   \n\n    Number of parameters (billion) [b]  \\\n0                                0.117   \n1                           0.340[135]   \n2                              11[139]   \n3                          ~0.340[142]   \n4                             1.5[146]   \n5                              175[46]   \n6                             2.7[152]   \n7                               6[155]   \n8                             530[158]   \n9                             260[159]   \n10                             52[161]   \n11                            1200[37]   \n12                            280[163]   \n13                            137[166]   \n14                             20[168]   \n15                             70[169]   \n16                            540[170]   \n17                            175[171]   \n18                            100[173]   \n19                            540[174]   \n20                            175[176]   \n21                                 120   \n22                             20[179]   \n23                             Unknown   \n24                             65[182]   \n25                          Unknown[f]   \n26                             13[185]   \n27                             40[186]   \n28                                  50   \n29                                1085   \n30                                  17   \n31                             Unknown   \n32                            340[195]   \n33                             70[197]   \n34                             Unknown   \n35                            7.3[200]   \n36                             Unknown   \n37                                 314   \n38                             Unknown   \n39                                46.7   \n40                                 141   \n41                                 2.7   \n42                             Unknown   \n43                                   7   \n44                             Unknown   \n45                                 136   \n46                                  13   \n47                                  70   \n48                 3.8B, 7B, 14B [216]   \n49  0.5b, 1.5b, 7b, 57b, and 72b [218]   \n50                                 340   \n\n                                          Corpus size  \\\n0                                                       \n1                              3.3 billion words[135]   \n2                              34 billion tokens[139]   \n3                                    33 billion words   \n4                 40GB[147] (~10 billion tokens)[148]   \n5                             300 billion tokens[148]   \n6                                        825 GiB[153]   \n7                                        825 GiB[153]   \n8                           338.6 billion tokens[158]   \n9                                                4 Tb   \n10                            400 billion tokens[161]   \n11                            1.6 trillion tokens[37]   \n12                            300 billion tokens[164]   \n13          1.56T words,[166] 168 billion tokens[164]   \n14                                       825 GiB[153]   \n15                      1.4 trillion tokens[169][164]   \n16                            768 billion tokens[169]   \n17                            180 billion tokens[172]   \n18                                         1.7TB[173]   \n19  38.5B tokens from webpages filtered for mathem...   \n20                    350 billion tokens (1.6TB)[177]   \n21                            106 billion tokens[178]   \n22                                  1.3 trillion[180]   \n23                                            Unknown   \n24                                  1.4 trillion[182]   \n25                                            Unknown   \n26                                                      \n27  1 trillion tokens, from RefinedWeb (filtered w...   \n28  363 billion token dataset based on Bloomberg's...   \n29                            329 billion tokens[191]   \n30                                1.5 trillion tokens   \n31                                            Unknown   \n32                           3.6 trillion tokens[195]   \n33                             2 trillion tokens[197]   \n34                                            Unknown   \n35                                            Unknown   \n36                                            Unknown   \n37                                            Unknown   \n38                                            Unknown   \n39                                            Unknown   \n40                                            Unknown   \n41                                        1.4T tokens   \n42                                            Unknown   \n43                                          6T tokens   \n44                                            Unknown   \n45                                         12T Tokens   \n46                                        380B Tokens   \n47                                         15T tokens   \n48                            3.3T, 4.8T, 4.8T Tokens   \n49                                          3T Tokens   \n50                                          9T Tokens   \n\n   Training cost (petaFLOP-day)                     License[c]  \\\n0                        1[133]                       MIT[134]   \n1                        9[136]                Apache 2.0[137]   \n2                                              Apache 2.0[140]   \n3                                              Apache 2.0[143]   \n4                                                     MIT[149]   \n5                     3640[150]                    proprietary   \n6                                                     MIT[154]   \n7                      200[156]                     Apache 2.0   \n8                                        Restricted web access   \n9                                                  Proprietary   \n10                                                        beta   \n11                     5600[37]                    Proprietary   \n12                    5833[165]                    Proprietary   \n13                    4110[167]                    Proprietary   \n14                     740[156]                     Apache 2.0   \n15                    6805[165]                    Proprietary   \n16                   29250[165]                    Proprietary   \n17                     310[156]     Non-commercial research[d]   \n18                                                  Apache 2.0   \n19                                                 Proprietary   \n20                                              Responsible AI   \n21                      unknown                   CC-BY-NC-4.0   \n22                                            proprietary[181]   \n23                                             privately-owned   \n24                    6300[183]     Non-commercial research[e]   \n25                      Unknown                    proprietary   \n26                     270[156]                     Apache 2.0   \n27                    2800[183]                Apache 2.0[189]   \n28                                                 Proprietary   \n29                                                 Proprietary   \n30                                                  Apache 2.0   \n31                                                 Proprietary   \n32                   85000[183]                    Proprietary   \n33                        21000                Llama 2 license   \n34                      Unknown                    Proprietary   \n35                                                  Apache 2.0   \n36                      Unknown                    Proprietary   \n37                      Unknown                     Apache 2.0   \n38                      Unknown                    Proprietary   \n39                      Unknown                     Apache 2.0   \n40                      Unknown                     Apache 2.0   \n41                     419[208]                            MIT   \n42                      Unknown                    Proprietary   \n43                      Unknown        Gemma Terms of Use[210]   \n44                      Unknown                    Proprietary   \n45                               Databricks Open Model License   \n46                                                               \n47                      100,000                Llama 3 license   \n48                                                         MIT   \n49                                                               \n50                      200,000      NVIDIA Open Model License   \n\n                                                Notes  \n0   First GPT model, decoder-only transformer. Tra...  \n1   An early and influential language model,[5] bu...  \n2   Base model for many Google projects, such as I...  \n3   An alternative to BERT; designed as encoder-on...  \n4   general-purpose model based on transformer arc...  \n5   A fine-tuned variant of GPT-3, termed GPT-3.5,...  \n6   The first of a series of free GPT-3 alternativ...  \n7                          GPT-3-style language model  \n8   Standard architecture but trained on a superco...  \n9   Chinese-language LLM. Ernie Bot is based on th...  \n10  Fine-tuned for desirable behavior in conversat...  \n11  Sparse mixture of experts model, making it mor...  \n12         Later developed into the Chinchilla model.  \n13  Specialized for response generation in convers...  \n14                 based on the Megatron architecture  \n15  Reduced-parameter model trained on more data. ...  \n16   Trained for ~60 days on ~6000 TPU v4 chips.[165]  \n17  GPT-3 architecture with some adaptations from ...  \n18  English-Russian model based on Microsoft's Meg...  \n19  For solving \"mathematical and scientific quest...  \n20  Essentially GPT-3 but trained on a multi-lingu...  \n21         Trained on scientific text and modalities.  \n22    bidirectional sequence-to-sequence architecture  \n23  A language model designed for live-streaming o...  \n24  Corpus has 20 languages. \"Overtrained\" (compar...  \n25  Available for ChatGPT Plus users and used in s...  \n26                   Trained with Chinchilla formula.  \n27                                                     \n28  Trained on financial data from proprietary sou...  \n29                                                     \n30                  Trained on crowdsourced open data  \n31                                  Multilingual[194]  \n32                     Was used in Bard chatbot.[196]  \n33                       1.7 million A100-hours.[198]  \n34                       Used in Claude chatbot.[199]  \n35                                                     \n36  Used in Claude chatbot. Has a context window o...  \n37  Used in Grok chatbot. Grok-1 has a context len...  \n38  Multimodal model, comes in three sizes. Used i...  \n39  Outperforms GPT-3.5 and Llama 2 70B on many be...  \n40                                              [207]  \n41  Trained on real and synthetic \"textbook-qualit...  \n42  Multimodal model, based on a Mixture-of-Expert...  \n43                                                     \n44  Includes three models, Haiku, Sonnet, and Opus...  \n45                      Training cost 10 million USD.  \n46  The largest model ever trained on CPU-only, on...  \n47  400B version yet to be released.[213] 70B vers...  \n48  Microsoft markets them as \"small language mode...  \n49                                                     \n50  Trained for 1 epoch. Trained on 6144 H100 GPUs...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Release date[a]</th>\n      <th>Developer</th>\n      <th>Number of parameters (billion) [b]</th>\n      <th>Corpus size</th>\n      <th>Training cost (petaFLOP-day)</th>\n      <th>License[c]</th>\n      <th>Notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GPT-1</td>\n      <td>June 2018</td>\n      <td>OpenAI</td>\n      <td>0.117</td>\n      <td></td>\n      <td>1[133]</td>\n      <td>MIT[134]</td>\n      <td>First GPT model, decoder-only transformer. Tra...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BERT</td>\n      <td>October 2018</td>\n      <td>Google</td>\n      <td>0.340[135]</td>\n      <td>3.3 billion words[135]</td>\n      <td>9[136]</td>\n      <td>Apache 2.0[137]</td>\n      <td>An early and influential language model,[5] bu...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>T5</td>\n      <td>October 2019</td>\n      <td>Google</td>\n      <td>11[139]</td>\n      <td>34 billion tokens[139]</td>\n      <td></td>\n      <td>Apache 2.0[140]</td>\n      <td>Base model for many Google projects, such as I...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>XLNet</td>\n      <td>June 2019</td>\n      <td>Google</td>\n      <td>~0.340[142]</td>\n      <td>33 billion words</td>\n      <td></td>\n      <td>Apache 2.0[143]</td>\n      <td>An alternative to BERT; designed as encoder-on...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GPT-2</td>\n      <td>February 2019</td>\n      <td>OpenAI</td>\n      <td>1.5[146]</td>\n      <td>40GB[147] (~10 billion tokens)[148]</td>\n      <td></td>\n      <td>MIT[149]</td>\n      <td>general-purpose model based on transformer arc...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>GPT-3</td>\n      <td>May 2020</td>\n      <td>OpenAI</td>\n      <td>175[46]</td>\n      <td>300 billion tokens[148]</td>\n      <td>3640[150]</td>\n      <td>proprietary</td>\n      <td>A fine-tuned variant of GPT-3, termed GPT-3.5,...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>GPT-Neo</td>\n      <td>March 2021</td>\n      <td>EleutherAI</td>\n      <td>2.7[152]</td>\n      <td>825 GiB[153]</td>\n      <td></td>\n      <td>MIT[154]</td>\n      <td>The first of a series of free GPT-3 alternativ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>GPT-J</td>\n      <td>June 2021</td>\n      <td>EleutherAI</td>\n      <td>6[155]</td>\n      <td>825 GiB[153]</td>\n      <td>200[156]</td>\n      <td>Apache 2.0</td>\n      <td>GPT-3-style language model</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Megatron-Turing NLG</td>\n      <td>October 2021[157]</td>\n      <td>Microsoft and Nvidia</td>\n      <td>530[158]</td>\n      <td>338.6 billion tokens[158]</td>\n      <td></td>\n      <td>Restricted web access</td>\n      <td>Standard architecture but trained on a superco...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Ernie 3.0 Titan</td>\n      <td>December 2021</td>\n      <td>Baidu</td>\n      <td>260[159]</td>\n      <td>4 Tb</td>\n      <td></td>\n      <td>Proprietary</td>\n      <td>Chinese-language LLM. Ernie Bot is based on th...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Claude[160]</td>\n      <td>December 2021</td>\n      <td>Anthropic</td>\n      <td>52[161]</td>\n      <td>400 billion tokens[161]</td>\n      <td></td>\n      <td>beta</td>\n      <td>Fine-tuned for desirable behavior in conversat...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>GLaM (Generalist Language Model)</td>\n      <td>December 2021</td>\n      <td>Google</td>\n      <td>1200[37]</td>\n      <td>1.6 trillion tokens[37]</td>\n      <td>5600[37]</td>\n      <td>Proprietary</td>\n      <td>Sparse mixture of experts model, making it mor...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Gopher</td>\n      <td>December 2021</td>\n      <td>DeepMind</td>\n      <td>280[163]</td>\n      <td>300 billion tokens[164]</td>\n      <td>5833[165]</td>\n      <td>Proprietary</td>\n      <td>Later developed into the Chinchilla model.</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>LaMDA (Language Models for Dialog Applications)</td>\n      <td>January 2022</td>\n      <td>Google</td>\n      <td>137[166]</td>\n      <td>1.56T words,[166] 168 billion tokens[164]</td>\n      <td>4110[167]</td>\n      <td>Proprietary</td>\n      <td>Specialized for response generation in convers...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>GPT-NeoX</td>\n      <td>February 2022</td>\n      <td>EleutherAI</td>\n      <td>20[168]</td>\n      <td>825 GiB[153]</td>\n      <td>740[156]</td>\n      <td>Apache 2.0</td>\n      <td>based on the Megatron architecture</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Chinchilla</td>\n      <td>March 2022</td>\n      <td>DeepMind</td>\n      <td>70[169]</td>\n      <td>1.4 trillion tokens[169][164]</td>\n      <td>6805[165]</td>\n      <td>Proprietary</td>\n      <td>Reduced-parameter model trained on more data. ...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>PaLM (Pathways Language Model)</td>\n      <td>April 2022</td>\n      <td>Google</td>\n      <td>540[170]</td>\n      <td>768 billion tokens[169]</td>\n      <td>29250[165]</td>\n      <td>Proprietary</td>\n      <td>Trained for ~60 days on ~6000 TPU v4 chips.[165]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>OPT (Open Pretrained Transformer)</td>\n      <td>May 2022</td>\n      <td>Meta</td>\n      <td>175[171]</td>\n      <td>180 billion tokens[172]</td>\n      <td>310[156]</td>\n      <td>Non-commercial research[d]</td>\n      <td>GPT-3 architecture with some adaptations from ...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>YaLM 100B</td>\n      <td>June 2022</td>\n      <td>Yandex</td>\n      <td>100[173]</td>\n      <td>1.7TB[173]</td>\n      <td></td>\n      <td>Apache 2.0</td>\n      <td>English-Russian model based on Microsoft's Meg...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Minerva</td>\n      <td>June 2022</td>\n      <td>Google</td>\n      <td>540[174]</td>\n      <td>38.5B tokens from webpages filtered for mathem...</td>\n      <td></td>\n      <td>Proprietary</td>\n      <td>For solving \"mathematical and scientific quest...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>BLOOM</td>\n      <td>July 2022</td>\n      <td>Large collaboration led by Hugging Face</td>\n      <td>175[176]</td>\n      <td>350 billion tokens (1.6TB)[177]</td>\n      <td></td>\n      <td>Responsible AI</td>\n      <td>Essentially GPT-3 but trained on a multi-lingu...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Galactica</td>\n      <td>November 2022</td>\n      <td>Meta</td>\n      <td>120</td>\n      <td>106 billion tokens[178]</td>\n      <td>unknown</td>\n      <td>CC-BY-NC-4.0</td>\n      <td>Trained on scientific text and modalities.</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>AlexaTM (Teacher Models)</td>\n      <td>November 2022</td>\n      <td>Amazon</td>\n      <td>20[179]</td>\n      <td>1.3 trillion[180]</td>\n      <td></td>\n      <td>proprietary[181]</td>\n      <td>bidirectional sequence-to-sequence architecture</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Neuro-sama</td>\n      <td>December 2022</td>\n      <td>Independent</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td></td>\n      <td>privately-owned</td>\n      <td>A language model designed for live-streaming o...</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>LLaMA (Large Language Model Meta AI)</td>\n      <td>February 2023</td>\n      <td>Meta AI</td>\n      <td>65[182]</td>\n      <td>1.4 trillion[182]</td>\n      <td>6300[183]</td>\n      <td>Non-commercial research[e]</td>\n      <td>Corpus has 20 languages. \"Overtrained\" (compar...</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>GPT-4</td>\n      <td>March 2023</td>\n      <td>OpenAI</td>\n      <td>Unknown[f]</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>proprietary</td>\n      <td>Available for ChatGPT Plus users and used in s...</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Cerebras-GPT</td>\n      <td>March 2023</td>\n      <td>Cerebras</td>\n      <td>13[185]</td>\n      <td></td>\n      <td>270[156]</td>\n      <td>Apache 2.0</td>\n      <td>Trained with Chinchilla formula.</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Falcon</td>\n      <td>March 2023</td>\n      <td>Technology Innovation Institute</td>\n      <td>40[186]</td>\n      <td>1 trillion tokens, from RefinedWeb (filtered w...</td>\n      <td>2800[183]</td>\n      <td>Apache 2.0[189]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>BloombergGPT</td>\n      <td>March 2023</td>\n      <td>Bloomberg L.P.</td>\n      <td>50</td>\n      <td>363 billion token dataset based on Bloomberg's...</td>\n      <td></td>\n      <td>Proprietary</td>\n      <td>Trained on financial data from proprietary sou...</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>PanGu-Σ</td>\n      <td>March 2023</td>\n      <td>Huawei</td>\n      <td>1085</td>\n      <td>329 billion tokens[191]</td>\n      <td></td>\n      <td>Proprietary</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>OpenAssistant[192]</td>\n      <td>March 2023</td>\n      <td>LAION</td>\n      <td>17</td>\n      <td>1.5 trillion tokens</td>\n      <td></td>\n      <td>Apache 2.0</td>\n      <td>Trained on crowdsourced open data</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Jurassic-2[193]</td>\n      <td>March 2023</td>\n      <td>AI21 Labs</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td></td>\n      <td>Proprietary</td>\n      <td>Multilingual[194]</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>PaLM 2 (Pathways Language Model 2)</td>\n      <td>May 2023</td>\n      <td>Google</td>\n      <td>340[195]</td>\n      <td>3.6 trillion tokens[195]</td>\n      <td>85000[183]</td>\n      <td>Proprietary</td>\n      <td>Was used in Bard chatbot.[196]</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Llama 2</td>\n      <td>July 2023</td>\n      <td>Meta AI</td>\n      <td>70[197]</td>\n      <td>2 trillion tokens[197]</td>\n      <td>21000</td>\n      <td>Llama 2 license</td>\n      <td>1.7 million A100-hours.[198]</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Claude 2</td>\n      <td>July 2023</td>\n      <td>Anthropic</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Proprietary</td>\n      <td>Used in Claude chatbot.[199]</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Mistral 7B</td>\n      <td>September 2023</td>\n      <td>Mistral AI</td>\n      <td>7.3[200]</td>\n      <td>Unknown</td>\n      <td></td>\n      <td>Apache 2.0</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Claude 2.1</td>\n      <td>November 2023</td>\n      <td>Anthropic</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Proprietary</td>\n      <td>Used in Claude chatbot. Has a context window o...</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Grok-1[202]</td>\n      <td>November 2023</td>\n      <td>x.AI</td>\n      <td>314</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Apache 2.0</td>\n      <td>Used in Grok chatbot. Grok-1 has a context len...</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Gemini 1.0</td>\n      <td>December 2023</td>\n      <td>Google DeepMind</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Proprietary</td>\n      <td>Multimodal model, comes in three sizes. Used i...</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>Mixtral 8x7B</td>\n      <td>December 2023</td>\n      <td>Mistral AI</td>\n      <td>46.7</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Apache 2.0</td>\n      <td>Outperforms GPT-3.5 and Llama 2 70B on many be...</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Mixtral 8x22B</td>\n      <td>April 2024</td>\n      <td>Mistral AI</td>\n      <td>141</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Apache 2.0</td>\n      <td>[207]</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>Phi-2</td>\n      <td>December 2023</td>\n      <td>Microsoft</td>\n      <td>2.7</td>\n      <td>1.4T tokens</td>\n      <td>419[208]</td>\n      <td>MIT</td>\n      <td>Trained on real and synthetic \"textbook-qualit...</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>Gemini 1.5</td>\n      <td>February 2024</td>\n      <td>Google DeepMind</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Proprietary</td>\n      <td>Multimodal model, based on a Mixture-of-Expert...</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>Gemma</td>\n      <td>February 2024</td>\n      <td>Google DeepMind</td>\n      <td>7</td>\n      <td>6T tokens</td>\n      <td>Unknown</td>\n      <td>Gemma Terms of Use[210]</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>Claude 3</td>\n      <td>March 2024</td>\n      <td>Anthropic</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Proprietary</td>\n      <td>Includes three models, Haiku, Sonnet, and Opus...</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>DBRX</td>\n      <td>March 2024</td>\n      <td>Databricks and Mosaic ML</td>\n      <td>136</td>\n      <td>12T Tokens</td>\n      <td></td>\n      <td>Databricks Open Model License</td>\n      <td>Training cost 10 million USD.</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>Fugaku-LLM</td>\n      <td>May 2024</td>\n      <td>Fujitsu, Tokyo Institute of Technology, etc.</td>\n      <td>13</td>\n      <td>380B Tokens</td>\n      <td></td>\n      <td></td>\n      <td>The largest model ever trained on CPU-only, on...</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Llama 3</td>\n      <td>April 2024</td>\n      <td>Meta AI</td>\n      <td>70</td>\n      <td>15T tokens</td>\n      <td>100,000</td>\n      <td>Llama 3 license</td>\n      <td>400B version yet to be released.[213] 70B vers...</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>Phi-3 family (mini, small, medium)</td>\n      <td>April 2024</td>\n      <td>Microsoft</td>\n      <td>3.8B, 7B, 14B [216]</td>\n      <td>3.3T, 4.8T, 4.8T Tokens</td>\n      <td></td>\n      <td>MIT</td>\n      <td>Microsoft markets them as \"small language mode...</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>Qwen2</td>\n      <td>June 2024</td>\n      <td>Alibaba Cloud</td>\n      <td>0.5b, 1.5b, 7b, 57b, and 72b [218]</td>\n      <td>3T Tokens</td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Nemotron-4</td>\n      <td>June 2024</td>\n      <td>Nvidia</td>\n      <td>340</td>\n      <td>9T Tokens</td>\n      <td>200,000</td>\n      <td>NVIDIA Open Model License</td>\n      <td>Trained for 1 epoch. Trained on 6144 H100 GPUs...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import re\n# Define a function to clean the text\ndef clean_text(text):\n    if isinstance(text, str):\n        return re.sub(r'\\[\\w+\\]', '', text)\n    return text\n\ndata.columns = [clean_text(col) for col in data.columns]\ndata_cleaned = data.map(clean_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-21T21:09:13.419039Z","iopub.execute_input":"2024-07-21T21:09:13.419475Z","iopub.status.idle":"2024-07-21T21:09:13.430846Z","shell.execute_reply.started":"2024-07-21T21:09:13.419422Z","shell.execute_reply":"2024-07-21T21:09:13.429585Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"data_cleaned","metadata":{"execution":{"iopub.status.busy":"2024-07-21T21:09:15.229074Z","iopub.execute_input":"2024-07-21T21:09:15.229766Z","iopub.status.idle":"2024-07-21T21:09:15.265529Z","shell.execute_reply.started":"2024-07-21T21:09:15.229643Z","shell.execute_reply":"2024-07-21T21:09:15.264254Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                               Name    Release date  \\\n0                                             GPT-1       June 2018   \n1                                              BERT    October 2018   \n2                                                T5    October 2019   \n3                                             XLNet       June 2019   \n4                                             GPT-2   February 2019   \n5                                             GPT-3        May 2020   \n6                                           GPT-Neo      March 2021   \n7                                             GPT-J       June 2021   \n8                               Megatron-Turing NLG    October 2021   \n9                                   Ernie 3.0 Titan   December 2021   \n10                                           Claude   December 2021   \n11                 GLaM (Generalist Language Model)   December 2021   \n12                                           Gopher   December 2021   \n13  LaMDA (Language Models for Dialog Applications)    January 2022   \n14                                         GPT-NeoX   February 2022   \n15                                       Chinchilla      March 2022   \n16                   PaLM (Pathways Language Model)      April 2022   \n17                OPT (Open Pretrained Transformer)        May 2022   \n18                                        YaLM 100B       June 2022   \n19                                          Minerva       June 2022   \n20                                            BLOOM       July 2022   \n21                                        Galactica   November 2022   \n22                         AlexaTM (Teacher Models)   November 2022   \n23                                       Neuro-sama   December 2022   \n24             LLaMA (Large Language Model Meta AI)   February 2023   \n25                                            GPT-4      March 2023   \n26                                     Cerebras-GPT      March 2023   \n27                                           Falcon      March 2023   \n28                                     BloombergGPT      March 2023   \n29                                          PanGu-Σ      March 2023   \n30                                    OpenAssistant      March 2023   \n31                                       Jurassic-2      March 2023   \n32               PaLM 2 (Pathways Language Model 2)        May 2023   \n33                                          Llama 2       July 2023   \n34                                         Claude 2       July 2023   \n35                                       Mistral 7B  September 2023   \n36                                       Claude 2.1   November 2023   \n37                                           Grok-1   November 2023   \n38                                       Gemini 1.0   December 2023   \n39                                     Mixtral 8x7B   December 2023   \n40                                    Mixtral 8x22B      April 2024   \n41                                            Phi-2   December 2023   \n42                                       Gemini 1.5   February 2024   \n43                                            Gemma   February 2024   \n44                                         Claude 3      March 2024   \n45                                             DBRX      March 2024   \n46                                       Fugaku-LLM        May 2024   \n47                                          Llama 3      April 2024   \n48               Phi-3 family (mini, small, medium)      April 2024   \n49                                            Qwen2       June 2024   \n50                                       Nemotron-4       June 2024   \n\n                                       Developer  \\\n0                                         OpenAI   \n1                                         Google   \n2                                         Google   \n3                                         Google   \n4                                         OpenAI   \n5                                         OpenAI   \n6                                     EleutherAI   \n7                                     EleutherAI   \n8                           Microsoft and Nvidia   \n9                                          Baidu   \n10                                     Anthropic   \n11                                        Google   \n12                                      DeepMind   \n13                                        Google   \n14                                    EleutherAI   \n15                                      DeepMind   \n16                                        Google   \n17                                          Meta   \n18                                        Yandex   \n19                                        Google   \n20       Large collaboration led by Hugging Face   \n21                                          Meta   \n22                                        Amazon   \n23                                   Independent   \n24                                       Meta AI   \n25                                        OpenAI   \n26                                      Cerebras   \n27               Technology Innovation Institute   \n28                                Bloomberg L.P.   \n29                                        Huawei   \n30                                         LAION   \n31                                     AI21 Labs   \n32                                        Google   \n33                                       Meta AI   \n34                                     Anthropic   \n35                                    Mistral AI   \n36                                     Anthropic   \n37                                          x.AI   \n38                               Google DeepMind   \n39                                    Mistral AI   \n40                                    Mistral AI   \n41                                     Microsoft   \n42                               Google DeepMind   \n43                               Google DeepMind   \n44                                     Anthropic   \n45                      Databricks and Mosaic ML   \n46  Fujitsu, Tokyo Institute of Technology, etc.   \n47                                       Meta AI   \n48                                     Microsoft   \n49                                 Alibaba Cloud   \n50                                        Nvidia   \n\n   Number of parameters (billion)   \\\n0                            0.117   \n1                            0.340   \n2                               11   \n3                           ~0.340   \n4                              1.5   \n5                              175   \n6                              2.7   \n7                                6   \n8                              530   \n9                              260   \n10                              52   \n11                            1200   \n12                             280   \n13                             137   \n14                              20   \n15                              70   \n16                             540   \n17                             175   \n18                             100   \n19                             540   \n20                             175   \n21                             120   \n22                              20   \n23                         Unknown   \n24                              65   \n25                         Unknown   \n26                              13   \n27                              40   \n28                              50   \n29                            1085   \n30                              17   \n31                         Unknown   \n32                             340   \n33                              70   \n34                         Unknown   \n35                             7.3   \n36                         Unknown   \n37                             314   \n38                         Unknown   \n39                            46.7   \n40                             141   \n41                             2.7   \n42                         Unknown   \n43                               7   \n44                         Unknown   \n45                             136   \n46                              13   \n47                              70   \n48                  3.8B, 7B, 14B    \n49   0.5b, 1.5b, 7b, 57b, and 72b    \n50                             340   \n\n                                          Corpus size  \\\n0                                                       \n1                                   3.3 billion words   \n2                                   34 billion tokens   \n3                                    33 billion words   \n4                           40GB (~10 billion tokens)   \n5                                  300 billion tokens   \n6                                             825 GiB   \n7                                             825 GiB   \n8                                338.6 billion tokens   \n9                                                4 Tb   \n10                                 400 billion tokens   \n11                                1.6 trillion tokens   \n12                                 300 billion tokens   \n13                    1.56T words, 168 billion tokens   \n14                                            825 GiB   \n15                                1.4 trillion tokens   \n16                                 768 billion tokens   \n17                                 180 billion tokens   \n18                                              1.7TB   \n19  38.5B tokens from webpages filtered for mathem...   \n20                         350 billion tokens (1.6TB)   \n21                                 106 billion tokens   \n22                                       1.3 trillion   \n23                                            Unknown   \n24                                       1.4 trillion   \n25                                            Unknown   \n26                                                      \n27  1 trillion tokens, from RefinedWeb (filtered w...   \n28  363 billion token dataset based on Bloomberg's...   \n29                                 329 billion tokens   \n30                                1.5 trillion tokens   \n31                                            Unknown   \n32                                3.6 trillion tokens   \n33                                  2 trillion tokens   \n34                                            Unknown   \n35                                            Unknown   \n36                                            Unknown   \n37                                            Unknown   \n38                                            Unknown   \n39                                            Unknown   \n40                                            Unknown   \n41                                        1.4T tokens   \n42                                            Unknown   \n43                                          6T tokens   \n44                                            Unknown   \n45                                         12T Tokens   \n46                                        380B Tokens   \n47                                         15T tokens   \n48                            3.3T, 4.8T, 4.8T Tokens   \n49                                          3T Tokens   \n50                                          9T Tokens   \n\n   Training cost (petaFLOP-day)                        License  \\\n0                             1                            MIT   \n1                             9                     Apache 2.0   \n2                                                   Apache 2.0   \n3                                                   Apache 2.0   \n4                                                          MIT   \n5                          3640                    proprietary   \n6                                                          MIT   \n7                           200                     Apache 2.0   \n8                                        Restricted web access   \n9                                                  Proprietary   \n10                                                        beta   \n11                         5600                    Proprietary   \n12                         5833                    Proprietary   \n13                         4110                    Proprietary   \n14                          740                     Apache 2.0   \n15                         6805                    Proprietary   \n16                        29250                    Proprietary   \n17                          310        Non-commercial research   \n18                                                  Apache 2.0   \n19                                                 Proprietary   \n20                                              Responsible AI   \n21                      unknown                   CC-BY-NC-4.0   \n22                                                 proprietary   \n23                                             privately-owned   \n24                         6300        Non-commercial research   \n25                      Unknown                    proprietary   \n26                          270                     Apache 2.0   \n27                         2800                     Apache 2.0   \n28                                                 Proprietary   \n29                                                 Proprietary   \n30                                                  Apache 2.0   \n31                                                 Proprietary   \n32                        85000                    Proprietary   \n33                        21000                Llama 2 license   \n34                      Unknown                    Proprietary   \n35                                                  Apache 2.0   \n36                      Unknown                    Proprietary   \n37                      Unknown                     Apache 2.0   \n38                      Unknown                    Proprietary   \n39                      Unknown                     Apache 2.0   \n40                      Unknown                     Apache 2.0   \n41                          419                            MIT   \n42                      Unknown                    Proprietary   \n43                      Unknown             Gemma Terms of Use   \n44                      Unknown                    Proprietary   \n45                               Databricks Open Model License   \n46                                                               \n47                      100,000                Llama 3 license   \n48                                                         MIT   \n49                                                               \n50                      200,000      NVIDIA Open Model License   \n\n                                                Notes  \n0   First GPT model, decoder-only transformer. Tra...  \n1   An early and influential language model, but e...  \n2   Base model for many Google projects, such as I...  \n3    An alternative to BERT; designed as encoder-only  \n4   general-purpose model based on transformer arc...  \n5   A fine-tuned variant of GPT-3, termed GPT-3.5,...  \n6   The first of a series of free GPT-3 alternativ...  \n7                          GPT-3-style language model  \n8   Standard architecture but trained on a superco...  \n9   Chinese-language LLM. Ernie Bot is based on th...  \n10  Fine-tuned for desirable behavior in conversat...  \n11  Sparse mixture of experts model, making it mor...  \n12         Later developed into the Chinchilla model.  \n13  Specialized for response generation in convers...  \n14                 based on the Megatron architecture  \n15  Reduced-parameter model trained on more data. ...  \n16        Trained for ~60 days on ~6000 TPU v4 chips.  \n17  GPT-3 architecture with some adaptations from ...  \n18  English-Russian model based on Microsoft's Meg...  \n19  For solving \"mathematical and scientific quest...  \n20  Essentially GPT-3 but trained on a multi-lingu...  \n21         Trained on scientific text and modalities.  \n22    bidirectional sequence-to-sequence architecture  \n23  A language model designed for live-streaming o...  \n24  Corpus has 20 languages. \"Overtrained\" (compar...  \n25  Available for ChatGPT Plus users and used in s...  \n26                   Trained with Chinchilla formula.  \n27                                                     \n28  Trained on financial data from proprietary sou...  \n29                                                     \n30                  Trained on crowdsourced open data  \n31                                       Multilingual  \n32                          Was used in Bard chatbot.  \n33                            1.7 million A100-hours.  \n34                            Used in Claude chatbot.  \n35                                                     \n36  Used in Claude chatbot. Has a context window o...  \n37  Used in Grok chatbot. Grok-1 has a context len...  \n38  Multimodal model, comes in three sizes. Used i...  \n39  Outperforms GPT-3.5 and Llama 2 70B on many be...  \n40                                                     \n41  Trained on real and synthetic \"textbook-qualit...  \n42  Multimodal model, based on a Mixture-of-Expert...  \n43                                                     \n44    Includes three models, Haiku, Sonnet, and Opus.  \n45                      Training cost 10 million USD.  \n46  The largest model ever trained on CPU-only, on...  \n47  400B version yet to be released. 70B version t...  \n48  Microsoft markets them as \"small language model\".  \n49                                                     \n50  Trained for 1 epoch. Trained on 6144 H100 GPUs...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Release date</th>\n      <th>Developer</th>\n      <th>Number of parameters (billion)</th>\n      <th>Corpus size</th>\n      <th>Training cost (petaFLOP-day)</th>\n      <th>License</th>\n      <th>Notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GPT-1</td>\n      <td>June 2018</td>\n      <td>OpenAI</td>\n      <td>0.117</td>\n      <td></td>\n      <td>1</td>\n      <td>MIT</td>\n      <td>First GPT model, decoder-only transformer. Tra...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BERT</td>\n      <td>October 2018</td>\n      <td>Google</td>\n      <td>0.340</td>\n      <td>3.3 billion words</td>\n      <td>9</td>\n      <td>Apache 2.0</td>\n      <td>An early and influential language model, but e...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>T5</td>\n      <td>October 2019</td>\n      <td>Google</td>\n      <td>11</td>\n      <td>34 billion tokens</td>\n      <td></td>\n      <td>Apache 2.0</td>\n      <td>Base model for many Google projects, such as I...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>XLNet</td>\n      <td>June 2019</td>\n      <td>Google</td>\n      <td>~0.340</td>\n      <td>33 billion words</td>\n      <td></td>\n      <td>Apache 2.0</td>\n      <td>An alternative to BERT; designed as encoder-only</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GPT-2</td>\n      <td>February 2019</td>\n      <td>OpenAI</td>\n      <td>1.5</td>\n      <td>40GB (~10 billion tokens)</td>\n      <td></td>\n      <td>MIT</td>\n      <td>general-purpose model based on transformer arc...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>GPT-3</td>\n      <td>May 2020</td>\n      <td>OpenAI</td>\n      <td>175</td>\n      <td>300 billion tokens</td>\n      <td>3640</td>\n      <td>proprietary</td>\n      <td>A fine-tuned variant of GPT-3, termed GPT-3.5,...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>GPT-Neo</td>\n      <td>March 2021</td>\n      <td>EleutherAI</td>\n      <td>2.7</td>\n      <td>825 GiB</td>\n      <td></td>\n      <td>MIT</td>\n      <td>The first of a series of free GPT-3 alternativ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>GPT-J</td>\n      <td>June 2021</td>\n      <td>EleutherAI</td>\n      <td>6</td>\n      <td>825 GiB</td>\n      <td>200</td>\n      <td>Apache 2.0</td>\n      <td>GPT-3-style language model</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Megatron-Turing NLG</td>\n      <td>October 2021</td>\n      <td>Microsoft and Nvidia</td>\n      <td>530</td>\n      <td>338.6 billion tokens</td>\n      <td></td>\n      <td>Restricted web access</td>\n      <td>Standard architecture but trained on a superco...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Ernie 3.0 Titan</td>\n      <td>December 2021</td>\n      <td>Baidu</td>\n      <td>260</td>\n      <td>4 Tb</td>\n      <td></td>\n      <td>Proprietary</td>\n      <td>Chinese-language LLM. Ernie Bot is based on th...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Claude</td>\n      <td>December 2021</td>\n      <td>Anthropic</td>\n      <td>52</td>\n      <td>400 billion tokens</td>\n      <td></td>\n      <td>beta</td>\n      <td>Fine-tuned for desirable behavior in conversat...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>GLaM (Generalist Language Model)</td>\n      <td>December 2021</td>\n      <td>Google</td>\n      <td>1200</td>\n      <td>1.6 trillion tokens</td>\n      <td>5600</td>\n      <td>Proprietary</td>\n      <td>Sparse mixture of experts model, making it mor...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Gopher</td>\n      <td>December 2021</td>\n      <td>DeepMind</td>\n      <td>280</td>\n      <td>300 billion tokens</td>\n      <td>5833</td>\n      <td>Proprietary</td>\n      <td>Later developed into the Chinchilla model.</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>LaMDA (Language Models for Dialog Applications)</td>\n      <td>January 2022</td>\n      <td>Google</td>\n      <td>137</td>\n      <td>1.56T words, 168 billion tokens</td>\n      <td>4110</td>\n      <td>Proprietary</td>\n      <td>Specialized for response generation in convers...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>GPT-NeoX</td>\n      <td>February 2022</td>\n      <td>EleutherAI</td>\n      <td>20</td>\n      <td>825 GiB</td>\n      <td>740</td>\n      <td>Apache 2.0</td>\n      <td>based on the Megatron architecture</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Chinchilla</td>\n      <td>March 2022</td>\n      <td>DeepMind</td>\n      <td>70</td>\n      <td>1.4 trillion tokens</td>\n      <td>6805</td>\n      <td>Proprietary</td>\n      <td>Reduced-parameter model trained on more data. ...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>PaLM (Pathways Language Model)</td>\n      <td>April 2022</td>\n      <td>Google</td>\n      <td>540</td>\n      <td>768 billion tokens</td>\n      <td>29250</td>\n      <td>Proprietary</td>\n      <td>Trained for ~60 days on ~6000 TPU v4 chips.</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>OPT (Open Pretrained Transformer)</td>\n      <td>May 2022</td>\n      <td>Meta</td>\n      <td>175</td>\n      <td>180 billion tokens</td>\n      <td>310</td>\n      <td>Non-commercial research</td>\n      <td>GPT-3 architecture with some adaptations from ...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>YaLM 100B</td>\n      <td>June 2022</td>\n      <td>Yandex</td>\n      <td>100</td>\n      <td>1.7TB</td>\n      <td></td>\n      <td>Apache 2.0</td>\n      <td>English-Russian model based on Microsoft's Meg...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Minerva</td>\n      <td>June 2022</td>\n      <td>Google</td>\n      <td>540</td>\n      <td>38.5B tokens from webpages filtered for mathem...</td>\n      <td></td>\n      <td>Proprietary</td>\n      <td>For solving \"mathematical and scientific quest...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>BLOOM</td>\n      <td>July 2022</td>\n      <td>Large collaboration led by Hugging Face</td>\n      <td>175</td>\n      <td>350 billion tokens (1.6TB)</td>\n      <td></td>\n      <td>Responsible AI</td>\n      <td>Essentially GPT-3 but trained on a multi-lingu...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Galactica</td>\n      <td>November 2022</td>\n      <td>Meta</td>\n      <td>120</td>\n      <td>106 billion tokens</td>\n      <td>unknown</td>\n      <td>CC-BY-NC-4.0</td>\n      <td>Trained on scientific text and modalities.</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>AlexaTM (Teacher Models)</td>\n      <td>November 2022</td>\n      <td>Amazon</td>\n      <td>20</td>\n      <td>1.3 trillion</td>\n      <td></td>\n      <td>proprietary</td>\n      <td>bidirectional sequence-to-sequence architecture</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Neuro-sama</td>\n      <td>December 2022</td>\n      <td>Independent</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td></td>\n      <td>privately-owned</td>\n      <td>A language model designed for live-streaming o...</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>LLaMA (Large Language Model Meta AI)</td>\n      <td>February 2023</td>\n      <td>Meta AI</td>\n      <td>65</td>\n      <td>1.4 trillion</td>\n      <td>6300</td>\n      <td>Non-commercial research</td>\n      <td>Corpus has 20 languages. \"Overtrained\" (compar...</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>GPT-4</td>\n      <td>March 2023</td>\n      <td>OpenAI</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>proprietary</td>\n      <td>Available for ChatGPT Plus users and used in s...</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Cerebras-GPT</td>\n      <td>March 2023</td>\n      <td>Cerebras</td>\n      <td>13</td>\n      <td></td>\n      <td>270</td>\n      <td>Apache 2.0</td>\n      <td>Trained with Chinchilla formula.</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Falcon</td>\n      <td>March 2023</td>\n      <td>Technology Innovation Institute</td>\n      <td>40</td>\n      <td>1 trillion tokens, from RefinedWeb (filtered w...</td>\n      <td>2800</td>\n      <td>Apache 2.0</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>BloombergGPT</td>\n      <td>March 2023</td>\n      <td>Bloomberg L.P.</td>\n      <td>50</td>\n      <td>363 billion token dataset based on Bloomberg's...</td>\n      <td></td>\n      <td>Proprietary</td>\n      <td>Trained on financial data from proprietary sou...</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>PanGu-Σ</td>\n      <td>March 2023</td>\n      <td>Huawei</td>\n      <td>1085</td>\n      <td>329 billion tokens</td>\n      <td></td>\n      <td>Proprietary</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>OpenAssistant</td>\n      <td>March 2023</td>\n      <td>LAION</td>\n      <td>17</td>\n      <td>1.5 trillion tokens</td>\n      <td></td>\n      <td>Apache 2.0</td>\n      <td>Trained on crowdsourced open data</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Jurassic-2</td>\n      <td>March 2023</td>\n      <td>AI21 Labs</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td></td>\n      <td>Proprietary</td>\n      <td>Multilingual</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>PaLM 2 (Pathways Language Model 2)</td>\n      <td>May 2023</td>\n      <td>Google</td>\n      <td>340</td>\n      <td>3.6 trillion tokens</td>\n      <td>85000</td>\n      <td>Proprietary</td>\n      <td>Was used in Bard chatbot.</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Llama 2</td>\n      <td>July 2023</td>\n      <td>Meta AI</td>\n      <td>70</td>\n      <td>2 trillion tokens</td>\n      <td>21000</td>\n      <td>Llama 2 license</td>\n      <td>1.7 million A100-hours.</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Claude 2</td>\n      <td>July 2023</td>\n      <td>Anthropic</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Proprietary</td>\n      <td>Used in Claude chatbot.</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Mistral 7B</td>\n      <td>September 2023</td>\n      <td>Mistral AI</td>\n      <td>7.3</td>\n      <td>Unknown</td>\n      <td></td>\n      <td>Apache 2.0</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Claude 2.1</td>\n      <td>November 2023</td>\n      <td>Anthropic</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Proprietary</td>\n      <td>Used in Claude chatbot. Has a context window o...</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Grok-1</td>\n      <td>November 2023</td>\n      <td>x.AI</td>\n      <td>314</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Apache 2.0</td>\n      <td>Used in Grok chatbot. Grok-1 has a context len...</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Gemini 1.0</td>\n      <td>December 2023</td>\n      <td>Google DeepMind</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Proprietary</td>\n      <td>Multimodal model, comes in three sizes. Used i...</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>Mixtral 8x7B</td>\n      <td>December 2023</td>\n      <td>Mistral AI</td>\n      <td>46.7</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Apache 2.0</td>\n      <td>Outperforms GPT-3.5 and Llama 2 70B on many be...</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Mixtral 8x22B</td>\n      <td>April 2024</td>\n      <td>Mistral AI</td>\n      <td>141</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Apache 2.0</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>Phi-2</td>\n      <td>December 2023</td>\n      <td>Microsoft</td>\n      <td>2.7</td>\n      <td>1.4T tokens</td>\n      <td>419</td>\n      <td>MIT</td>\n      <td>Trained on real and synthetic \"textbook-qualit...</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>Gemini 1.5</td>\n      <td>February 2024</td>\n      <td>Google DeepMind</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Proprietary</td>\n      <td>Multimodal model, based on a Mixture-of-Expert...</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>Gemma</td>\n      <td>February 2024</td>\n      <td>Google DeepMind</td>\n      <td>7</td>\n      <td>6T tokens</td>\n      <td>Unknown</td>\n      <td>Gemma Terms of Use</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>Claude 3</td>\n      <td>March 2024</td>\n      <td>Anthropic</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Unknown</td>\n      <td>Proprietary</td>\n      <td>Includes three models, Haiku, Sonnet, and Opus.</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>DBRX</td>\n      <td>March 2024</td>\n      <td>Databricks and Mosaic ML</td>\n      <td>136</td>\n      <td>12T Tokens</td>\n      <td></td>\n      <td>Databricks Open Model License</td>\n      <td>Training cost 10 million USD.</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>Fugaku-LLM</td>\n      <td>May 2024</td>\n      <td>Fujitsu, Tokyo Institute of Technology, etc.</td>\n      <td>13</td>\n      <td>380B Tokens</td>\n      <td></td>\n      <td></td>\n      <td>The largest model ever trained on CPU-only, on...</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Llama 3</td>\n      <td>April 2024</td>\n      <td>Meta AI</td>\n      <td>70</td>\n      <td>15T tokens</td>\n      <td>100,000</td>\n      <td>Llama 3 license</td>\n      <td>400B version yet to be released. 70B version t...</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>Phi-3 family (mini, small, medium)</td>\n      <td>April 2024</td>\n      <td>Microsoft</td>\n      <td>3.8B, 7B, 14B</td>\n      <td>3.3T, 4.8T, 4.8T Tokens</td>\n      <td></td>\n      <td>MIT</td>\n      <td>Microsoft markets them as \"small language model\".</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>Qwen2</td>\n      <td>June 2024</td>\n      <td>Alibaba Cloud</td>\n      <td>0.5b, 1.5b, 7b, 57b, and 72b</td>\n      <td>3T Tokens</td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Nemotron-4</td>\n      <td>June 2024</td>\n      <td>Nvidia</td>\n      <td>340</td>\n      <td>9T Tokens</td>\n      <td>200,000</td>\n      <td>NVIDIA Open Model License</td>\n      <td>Trained for 1 epoch. Trained on 6144 H100 GPUs...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
